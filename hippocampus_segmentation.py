# -*- coding: utf-8 -*-
"""Hippocampus_segmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sGzu4RQn4mCaDsSHu096Cdceg96KKucT
"""

# Installing torchio
!pip install --quiet torchio

# Downloading the relevant package
import numpy as np
import matplotlib.pyplot as plt
import torch
import torchvision
import torchio as tio
import torch.nn as nn
import os
import re
from google.colab import drive
drive.mount('/content/drive')
import glob
import nibabel as nib
import cv2
from torch.utils.data import random_split
import torch.optim as optim
from torchio import SubjectsDataset, SubjectsLoader
import torchvision.models as models
import torch.nn.functional as F
from torch.optim.lr_scheduler import ReduceLROnPlateau
import warnings
warnings.filterwarnings(
    "ignore",
    message="Using TorchIO images without a torchio.SubjectsLoader"
)
warnings.filterwarnings("ignore", message=".*maximum displacement is larger than the coarse grid spacing.*")
from skimage.metrics import hausdorff_distance
from scipy.spatial.distance import cdist

img = nib.load('/content/drive/MyDrive/AMLIH/AMLH_Medical_image/imagesTrain/hippocampus_070.nii.gz')
label = nib.load('/content/drive/MyDrive/AMLIH/AMLH_Medical_image/labelsTrain/hippocampus_070.nii.gz')
img = img.get_fdata()
label = label.get_fdata()

print(img.shape)
print(label.shape)

def plot_slices(img,num_slices):
  '''
  A function that visualises 2D slices of the 3D scan
  args:
      img - A 3D numpy array of the scan
      num_slices - The number of slices to visualise
  '''
  slices = np.linspace(0, img.shape[2],num_slices, endpoint = False).astype(int)
  fig, ax = plt.subplots(1, num_slices, figsize = (18,9))
  ax = ax.flatten()
  for i, slice in enumerate(slices):
    ax[i].imshow(img[:,:, slice], cmap = 'gray')
    ax[i].axis('off')
    ax[i].set_title(f'image slice: {str(slice)}')
  plt.tight_layout()

plot_slices(img, 10)

def mask_overlay(img, mask, slice, alpha):
  ''' A function that visulises the overlay between the image and corresponding mask
      args:
          img - Orginal 3D image in numpy format
          mask - Orginal 3D mask in numpy format
          slice - The slice you want to visualise
          alpha - The transparancy of the mask overlay
  '''
  img, mask = img[:,:,slice].astype(np.float32), mask[:,:,slice].astype(np.float32)
  colour_map = {1: np.array((0,255, 0), dtype = np.uint8),
                2: np.array((0,0,255), dtype = np.uint8)}

  masks = []
  for x in range(1,3):
    binary_mask = (mask == x).astype(np.uint8) * 255
    coloured_mask = cv2.cvtColor(binary_mask, cv2.COLOR_GRAY2BGR)
    coloured_array = np.full_like(coloured_mask, colour_map[x])
    coloured_mask = cv2.bitwise_and(coloured_mask, coloured_array)
    masks.append(coloured_mask)
  blended_mask = sum(masks)

  # Scale the image data to the range 0-255 for blending if it's not already
  if img.max() > 1 or img.min() < 0:
      img = (img - img.min()) / (img.max() - img.min()) * 255

  img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_GRAY2BGR)
  blended_mask = blended_mask.astype(np.uint8)
  image = cv2.addWeighted(img, 1 - alpha, blended_mask, alpha, 0)

  plt.imshow(image)


mask_overlay(img, label, 9, 0.3)

def Mask_Overlay(img, mask, slice, alpha):
  ''' A function that visulises the overlay between the image and corresponding mask
      args:
          img - Orginal 3D image in numpy format
          mask - Orginal 3D mask in numpy format
          slice - The slice you want to visualise
          alpha - The transparancy of the mask overlay
      returns:
        image that will be visalised in the image series function
  '''
  img, mask = img[:,:,slice].astype(np.float32), mask[:,:,slice].astype(np.float32)
  colour_map = {1: np.array((0,255, 0), dtype = np.uint8),
                2: np.array((0,0,255), dtype = np.uint8)}

  masks = []
  for x in range(1,3):
    binary_mask = (mask == x).astype(np.uint8) * 255
    coloured_mask = cv2.cvtColor(binary_mask, cv2.COLOR_GRAY2BGR)
    coloured_array = np.full_like(coloured_mask, colour_map[x])
    coloured_mask = cv2.bitwise_and(coloured_mask, coloured_array)
    masks.append(coloured_mask)
  blended_mask = sum(masks)

  # Scale the image data to the range 0-255 for blending if it's not already
  if img.max() > 1 or img.min() < 0:
      img = (img - img.min()) / (img.max() - img.min()) * 255

  img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_GRAY2BGR)
  blended_mask = blended_mask.astype(np.uint8)
  image = cv2.addWeighted(img, 1 - alpha, blended_mask, alpha, 0)

  return image


def image_series(img, mask, num, alpha):
  ''' A function that returns a 2D image series moving through the 3D image
      args:
          img - 3D image in numpy format
          mask - 3D mask in numpy format
          num - Number of 2D slices you to visualise
          alpha - The transparancy of the mask
      returns:
        An image series of you moving through 3D image
  '''
  slices = np.linspace(0, img.shape[2],num, endpoint = False).astype(int)
  fig, ax = plt.subplots(1, num, figsize = (20,15))
  orginal_img = img
  for i, x in enumerate(slices):
    img = Mask_Overlay(orginal_img, mask, x ,alpha)
    ax[i].imshow(img)
    ax[i].axis('off')
    ax[i].set_title(f'slice: {str(x)}')
  plt.tight_layout()


image_series(img, label, 10, 0.4)

# Creating a 3D unet Architecture with attention gates
class Encoder_block(nn.Module):
  def __init__(self, channels_in, channels_out):
    '''Args:
          channels_in - Number of channels inputed into the model or pevious encoder block.
          1 for greyscale CT scans (model input)
          Channels_out - Base numbers of feature maps that will be upscaled throughut the encder or
          passed to the decoder if its the final encoder block

    '''
    super(Encoder_block, self).__init__()

    # the encoder compresses spatial dimentions while increasing feature depth
    # Each encoder block consist of 2x (3D convolution and Relu activation) followed by Max pooling
    # Max pooling half spacial demintions - D/H/W
    self.conv = nn.Sequential(
        nn.Conv3d(channels_in, channels_out, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU(inplace = True),
        nn.Conv3d(channels_out, channels_out, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU(inplace = True)
        )
    self.pool = nn.MaxPool3d(2,2)

  # Defining how data will move through the encoder
  def forward(self, x):
    '''Args:
          x (torch.Tensor) - Input Tensor shape [B, Channels_in, D, H, W]
          Returns - x: Channels_out number of feature maps that will go to decoder
                       via the skip connection
                    p: Downsampled Feature maps after pooling. These will travel down the encoder
    '''
    x = self.conv(x)
    p = self.pool(x)
    return x , p

class attention_gate(nn.Module):
  def __init__(self, channels_in, skip_channels, channels_out):
    super(attention_gate, self).__init__()

    self.Wg = nn.Sequential(
        nn.Conv3d(channels_in, channels_out, kernel_size = 3, stride = 1, padding = 1),
        nn.BatchNorm3d(channels_out)
    )

    self.Ws = nn.Sequential(
        nn.Conv3d(skip_channels, channels_out, kernel_size = 3, stride = 1, padding = 1),
        nn.BatchNorm3d(channels_out)
    )

    self.ReLU = nn.ReLU(inplace = True)
    self.output = nn.Sequential(
        nn.Conv3d(channels_out, channels_out, kernel_size = 3, stride = 1, padding = 1),
        nn.Sigmoid()
    )

  def forward(self, x, skip):
    wg = self.Wg(x)
    ws = self.Ws(skip)
    out = self.ReLU(wg + ws)
    out = self.output(out)
    return out * skip

# Defining the decoder block - Upsamples back to the orginal size using ConvTranspose3D
# Each stage halves feature depth ad doubles spacial information
class Decoder_block(nn.Module):
  def __init__(self, channels_in, skip_channels, channels_out):
    '''Args:
          channels_in: Input feature maps from the bottleneck or previous decoder block
          Skip_channels: feature maps from the encoder coming through the skip connection
          channels_out: feature maps outputted to the next decder block or final output layer

    '''
    super(Decoder_block, self).__init__()

    self.upsample = nn.ConvTranspose3d(channels_in, channels_out, kernel_size=2, stride = 2)
    self.attention_gate = attention_gate(channels_out, skip_channels, channels_out)
    self.conv = nn.Sequential(
         nn.Conv3d(channels_out + skip_channels, channels_out, kernel_size=3, stride = 1, padding = 1),
         nn.ReLU(inplace = True),
         nn.Conv3d(channels_out, channels_out, kernel_size=3, stride = 1, padding = 1),
         nn.ReLU(inplace = True)
    )

  def forward(self, x, skip):
    '''Args:
          x - Features maps from bottleneck or previous decoder block
          Skip - Features maps coming from the encoder via the skip connection
       returns:
          Upsampled feature maps the will be used in the next decoder block or ouput layer
    '''
    x = self.upsample(x)
    x = self.attention_gate(x , skip)
    x = torch.cat([x, skip], dim=1)
    x = self.conv(x)
    return x


# Defining 3D Unet architecture - Data flows through 4 encoder blocoks reducing spacial size (D/H/W)
# Skip connections send feature maps to decoder by max pooling in each block
# Decoder upsamples back to orginal image size, feature maps are downscaled
class Attention_3D_unet(nn.Module):
  def __init__(self, channels_in, channels_out):
    '''Args:
          channels_in - num channels inputted into first encoder block, 1 for greyscale CT scan
          Channels_out - Inital number of feature maps that will be outputted by first encoder block.
          This will be upscaled during the encoder and and downscaled during the decoder.
    '''
    super(Attention_3D_unet, self).__init__()

    # 4 encoder blocks - upscaled feature maps and reduces spacial deimentions of image
    # First 3 send features maps to corresponding decoder block, last sends feature maps to the bottlebeck
    self.Encoder_block1 = Encoder_block(channels_in, channels_out)
    self.Encoder_block2 = Encoder_block(channels_out, channels_out * 2)
    self.Encoder_block3 = Encoder_block(channels_out * 2, channels_out * 4)
    self.Encoder_block4 = Encoder_block(channels_out * 4, channels_out * 8)

    # Bottleneck learns compressed high-level features at the lowest resolution between encoder and decoder.
    self.bottle_neck = nn.Sequential(
        nn.Conv3d(channels_out * 8, channels_out * 16, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU(inplace = True),
        nn.Conv3d(channels_out * 16, channels_out * 16, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU()
    )

    # Decoder architecure unsamples image spacial dimentions and reduces feature maps number
    # 1st decoder block inputs feature maps from bottleneck, next 3 lays recieves feature maps
    # from previous decoder block. Feature then fed to output layer
    self.decoder_block4 = Decoder_block(channels_out * 16, channels_out * 8, channels_out * 8)
    self.decoder_block3 = Decoder_block(channels_out * 8, channels_out * 4, channels_out * 4)
    self.decoder_block2 = Decoder_block(channels_out * 4, channels_out * 2, channels_out * 2)
    self.decoder_block1 = Decoder_block(channels_out * 2, channels_out , channels_out)

    # Map decoder output to 3 output chanbels, blackground left hypothalamus and right hypothalamus
    self.output = nn.Conv3d(channels_out, 3, kernel_size=3, stride = 1, padding = 1)


  def forward(self, x):
    '''Args:
          x (Torch.Tensor) - input tensor of shape [B, channels_in, D,H,W]
       returns:
          Torch.Tensor: Segmentation logits of shape [Batch, 3, Depth, Height, widith]
                         3-class output - Background, left hypothalamus, right hypothalamus
    '''
    x1, p1 = self.Encoder_block1(x)
    x2, p2 = self.Encoder_block2(p1)
    x3, p3 = self.Encoder_block3(p2)
    x4, p4 = self.Encoder_block4(p3)

    x5 = self.bottle_neck(p4)

    d4 = self.decoder_block4(x5, x4)
    d3 = self.decoder_block3(d4, x3)
    d2 = self.decoder_block2(d3, x2)
    d1 = self.decoder_block1(d2, x1)

    output = self.output(d1)

    return output

# Creating a 3D unet Architecture using Encoder-Decoder structure with architecture
class Basic_Unet_3D(nn.Module):
  def __init__(self, channels_in, channels_out):
    '''Args:
          channels_in - Number of channels inputed into the model or pevious encoder block.
          1 for greyscale CT scans (model input)
          Channels_out - Base numbers of feature maps that will be upscaled throughut the encder or
          passed to the decoder if its the final encoder block

    '''
    super(Basic_Unet_3D, self).__init__()

    # the encoder compresses spatial dimentions while increasing feature depth
    # Each encoder block consist of 2x (3D convolution and Relu activation) followed by Max pooling
    # Max pooling half spacial demintions - D/H/W
    self.Encoder_block1 = Encoder_block(channels_in, channels_out)
    self.Encoder_block2 = Encoder_block(channels_out, channels_out * 2)
    self.Encoder_block3 = Encoder_block(channels_out * 2, channels_out * 4)
    self.Encoder_block4 = Encoder_block(channels_out * 4, channels_out * 8)

    # Bottleneck learns compressed high-level features at the lowest resolution between encoder and decoder.
    self.bottle_neck = nn.Sequential(
        nn.Conv3d(channels_out * 8, channels_out * 16, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU(inplace = True),
        nn.Conv3d(channels_out * 16, channels_out * 16, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU()
    )

    # Decoder architecure unsamples image spacial dimentions and reduces feature maps number
    # 1st decoder block inputs feature maps from bottleneck, next 3 lays recieves feature maps
    # from previous decoder block. Feature then fed to output layer
    self.decoder_block4 = Decoder_block(channels_out * 16, channels_out * 8, channels_out * 8)
    self.decoder_block3 = Decoder_block(channels_out * 8, channels_out * 4, channels_out * 4)
    self.decoder_block2 = Decoder_block(channels_out * 4, channels_out * 2, channels_out * 2)
    self.decoder_block1 = Decoder_block(channels_out * 2, channels_out , channels_out)

    # Map decoder output to 3 output chanbels, blackground left hypothalamus and right hypothalamus
    self.output = nn.Conv3d(channels_out, 3, kernel_size=3, stride = 1, padding = 1)


  def forward(self, x):
    '''Args:
          x (Torch.Tensor) - input tensor of shape [B, channels_in, D,H,W]
       returns:
          Torch.Tensor: Segmentation logits of shape [Batch, 3, Depth, Height, widith]
                         3-class output - Background, left hypothalamus and right hypothalamus
    '''
    x1, p1 = self.Encoder_block1(x)
    x2, p2 = self.Encoder_block2(p1)
    x3, p3 = self.Encoder_block3(p2)
    x4, p4 = self.Encoder_block4(p3)

    x5 = self.bottle_neck(p4)

    d4 = self.decoder_block4(x5, x4)
    d3 = self.decoder_block3(d4, x3)
    d2 = self.decoder_block2(d3, x2)
    d1 = self.decoder_block1(d2, x1)

    output = self.output(d1)

    return output

class channel_attention(nn.Module):
  def __init__(self, channels_in, reduction_ratio = 16):
    super(channel_attention, self).__init__()

    self.mlp = nn.Sequential(
        nn.Linear(channels_in, channels_in//reduction_ratio, bias = False),
        nn.ReLU(inplace = True),
        nn.Linear(channels_in// reduction_ratio, channels_in, bias = False)
    )
    self.avg_pool = nn.AdaptiveAvgPool3d(1)
    self.max_pool = nn.AdaptiveMaxPool3d(1)
    self.sigmoid = nn.Sigmoid()

  def forward(self, x):
    b,c,_,_,_= x.size()
    avg_out = self.mlp(self.avg_pool(x).view(b,c))
    max_out = self.mlp(self.max_pool(x).view(b,c))
    out = avg_out + max_out
    scale = self.sigmoid(out).view(b,c,1,1,1)
    return x * scale

class spatial_attention(nn.Module):
  def __init__(self, kernel_size = 7):
    super(spatial_attention, self).__init__()
    padding = (kernel_size -1) // 2

    self.conv = nn.Conv3d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)
    self.sigmoid = nn.Sigmoid()

  def forward(self, x):

    avg_out = torch.mean(x, dim=1, keepdim= True)
    max_out, _ = torch.max(x, dim = 1, keepdim= True)

    x_cat = torch.cat([avg_out, max_out], dim=1)

    attention = self.sigmoid(self.conv(x_cat))

    return x * attention

class CBAM(nn.Module):
  def __init__(self, channels_in, reduction_ratio = 16, kernel_size =7):
    super(CBAM, self).__init__()

    self.channel_attention = channel_attention(channels_in, reduction_ratio)
    self.spatial_attention = spatial_attention(kernel_size)

  def forward(self, x):
    x = self.channel_attention(x)
    x = self.spatial_attention(x)
    return x

# Creating a 3D unet Architecture with attention gates
class Encoder_block(nn.Module):
  def __init__(self, channels_in, channels_out):
    '''Args:
          channels_in - Number of channels inputed into the model or pevious encoder block.
          1 for greyscale CT scans (model input)
          Channels_out - Base numbers of feature maps that will be upscaled throughut the encder or
          passed to the decoder if its the final encoder block

    '''
    super(Encoder_block, self).__init__()

    # the encoder compresses spatial dimentions while increasing feature depth
    # Each encoder block consist of 2x (3D convolution and Relu activation) followed by Max pooling
    # Max pooling half spacial demintions - D/H/W
    self.conv = nn.Sequential(
        nn.Conv3d(channels_in, channels_out, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU(inplace = True),
        nn.Conv3d(channels_out, channels_out, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU(inplace = True)
        )
    self.pool = nn.MaxPool3d(2,2)
    self.CBAM = CBAM(channels_out)

  # Defining how data will move through the encoder
  def forward(self, x):
    '''Args:
          x (torch.Tensor) - Input Tensor shape [B, Channels_in, D, H, W]
          Returns - x: Channels_out number of feature maps that will go to decoder
                       via the skip connection
                    p: Downsampled Feature maps after pooling. These will travel down the encoder
    '''
    x = self.conv(x)
    x = self.CBAM(x)
    p = self.pool(x)
    return x , p

class attention_gate(nn.Module):
  def __init__(self, channels_in, skip_channels, channels_out):
    super(attention_gate, self).__init__()

    self.Wg = nn.Sequential(
        nn.Conv3d(channels_in, channels_out, kernel_size = 3, stride = 1, padding = 1),
        nn.BatchNorm3d(channels_out)
    )

    self.Ws = nn.Sequential(
        nn.Conv3d(skip_channels, channels_out, kernel_size = 3, stride = 1, padding = 1),
        nn.BatchNorm3d(channels_out)
    )

    self.ReLU = nn.ReLU(inplace = True)
    self.output = nn.Sequential(
        nn.Conv3d(channels_out, channels_out, kernel_size = 3, stride = 1, padding = 1),
        nn.Sigmoid()
    )

  def forward(self, x, skip):
    wg = self.Wg(x)
    ws = self.Ws(skip)
    out = self.ReLU(wg + ws)
    out = self.output(out)
    return out * skip

# Defining the decoder block - Upsamples back to the orginal size using ConvTranspose3D
# Each stage halves feature depth ad doubles spacial information
class Decoder_block(nn.Module):
  def __init__(self, channels_in, skip_channels, channels_out):
    '''Args:
          channels_in: Input feature maps from the bottleneck or previous decoder block
          Skip_channels: feature maps from the encoder coming through the skip connection
          channels_out: feature maps outputted to the next decder block or final output layer

    '''
    super(Decoder_block, self).__init__()

    self.upsample = nn.ConvTranspose3d(channels_in, channels_out, kernel_size=2, stride = 2)
    self.attention_gate = attention_gate(channels_out, skip_channels, channels_out)
    self.conv = nn.Sequential(
         nn.Conv3d(channels_out + skip_channels, channels_out, kernel_size=3, stride = 1, padding = 1),
         nn.ReLU(inplace = True),
         nn.Conv3d(channels_out, channels_out, kernel_size=3, stride = 1, padding = 1),
         nn.ReLU(inplace = True)
    )
    self.CBAM = CBAM(channels_out)

  def forward(self, x, skip):
    '''Args:
          x - Features maps from bottleneck or previous decoder block
          Skip - Features maps coming from the encoder via the skip connection
       returns:
          Upsampled feature maps the will be used in the next decoder block or ouput layer
    '''
    x = self.upsample(x)
    x = self.attention_gate(x , skip)
    x = torch.cat([x, skip], dim=1)
    x = self.conv(x)
    x = self.CBAM(x)
    return x


# Defining 3D Unet architecture - Data flows through 4 encoder blocoks reducing spacial size (D/H/W)
# Skip connections send feature maps to decoder by max pooling in each block
# Decoder upsamples back to orginal image size, feature maps are downscaled
class full_Attention_3D_unet(nn.Module):
  def __init__(self, channels_in, channels_out):
    '''Args:
          channels_in - num channels inputted into first encoder block, 1 for greyscale CT scan
          Channels_out - Inital number of feature maps that will be outputted by first encoder block.
          This will be upscaled during the encoder and and downscaled during the decoder.
    '''
    super(full_Attention_3D_unet, self).__init__()

    # 4 encoder blocks - upscaled feature maps and reduces spacial deimentions of image
    # First 3 send features maps to corresponding decoder block, last sends feature maps to the bottlebeck
    self.Encoder_block1 = Encoder_block(channels_in, channels_out)
    self.Encoder_block2 = Encoder_block(channels_out, channels_out * 2)
    self.Encoder_block3 = Encoder_block(channels_out * 2, channels_out * 4)
    self.Encoder_block4 = Encoder_block(channels_out * 4, channels_out * 8)

    # Bottleneck learns compressed high-level features at the lowest resolution between encoder and decoder.
    self.bottle_neck = nn.Sequential(
        nn.Conv3d(channels_out * 8, channels_out * 16, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU(inplace = True),
        nn.Conv3d(channels_out * 16, channels_out * 16, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU()
    )

    # Decoder architecure unsamples image spacial dimentions and reduces feature maps number
    # 1st decoder block inputs feature maps from bottleneck, next 3 lays recieves feature maps
    # from previous decoder block. Feature then fed to output layer
    self.decoder_block4 = Decoder_block(channels_out * 16, channels_out * 8, channels_out * 8)
    self.decoder_block3 = Decoder_block(channels_out * 8, channels_out * 4, channels_out * 4)
    self.decoder_block2 = Decoder_block(channels_out * 4, channels_out * 2, channels_out * 2)
    self.decoder_block1 = Decoder_block(channels_out * 2, channels_out , channels_out)

    # Map decoder output to 3 output chanbels, blackground left hypothalamus and right hypothalamus
    self.output = nn.Conv3d(channels_out, 3, kernel_size=3, stride = 1, padding = 1)


  def forward(self, x):
    '''Args:
          x (Torch.Tensor) - input tensor of shape [B, channels_in, D,H,W]
       returns:
          Torch.Tensor: Segmentation logits of shape [Batch, 3, Depth, Height, widith]
                         3-class output - Background, left hypothalamus, right hypothalamus
    '''
    x1, p1 = self.Encoder_block1(x)
    x2, p2 = self.Encoder_block2(p1)
    x3, p3 = self.Encoder_block3(p2)
    x4, p4 = self.Encoder_block4(p3)

    x5 = self.bottle_neck(p4)

    d4 = self.decoder_block4(x5, x4)
    d3 = self.decoder_block3(d4, x3)
    d2 = self.decoder_block2(d3, x2)
    d1 = self.decoder_block1(d2, x1)

    output = self.output(d1)

    return output

class PatchEmbeddings(nn.Module):
  def __init__(self, channels_in, patch_size, emb_size):
    super(PatchEmbeddings, self).__init__()

    # Setting the patch size for later use
    self.patch_size = patch_size

    # This Conv3D operation splits the 3D image into non-overlapping patches
    # and projects each patch into an embedding of dimension `emb_size`
    self.embedding = nn.Sequential(
        nn.Conv3d(channels_in, emb_size, kernel_size=patch_size, stride=patch_size),
        # Rearrange shape from (B, C, D, H, W) to (B, N, C), where N is number of patches
        rearrange('b c d h w -> b (d h w) c')
    )

  def forward(self, x):
    # Apply convolution and rearrange into patch embeddings
    return self.embedding(x)

class PositionalEmbedding(nn.Module):
  def __init__(self, seq_len, emb_size, base=10000):
    super(PositionalEmbedding, self).__init__()

    # Create a tensor with positions (0 to seq_len-1), shape: (seq_len, 1)
    position = torch.arange(seq_len, dtype=torch.float32).unsqueeze(1)

    # Compute denominator terms for sine/cosine functions
    # Using log space to handle wide range of frequencies
    div_term = torch.exp(torch.arange(0, emb_size, 2) * (-math.log(base) / emb_size))

    # Initialize positional embedding matrix of shape (seq_len, emb_size)
    pe = torch.zeros(seq_len, emb_size)

    # Apply sine to even indices in the embedding
    pe[:, 0::2] = torch.sin(position * div_term)

    # Apply cosine to odd indices in the embedding
    pe[:, 1::2] = torch.cos(position * div_term)

    # Add batch dimension: (1, seq_len, emb_size)
    pe = pe.unsqueeze(0)

    # Register pe as a buffer so it's part of model state but not a trainable parameter
    self.register_buffer('pe', pe)

  def forward(self, x):
    # Add positional embeddings to input (broadcasts over batch)
    return x + self.pe[:, :x.size(1)]

class multihead(nn.Module):
  def __init__(self, emb_size, num_head):
    super(multihead, self).__init__()

    # Store embedding size and number of heads
    self.emb_size = emb_size
    self.num_head = num_head

    # Create separate linear projections for keys, queries, and values
    self.key = nn.Linear(emb_size, emb_size)
    self.value = nn.Linear(emb_size, emb_size)
    self.query = nn.Linear(emb_size, emb_size)

    # Dropout layer for attention weights
    self.att_dr = nn.Dropout(0.1)

  def forward(self, x):
    # Project input x to keys, queries, and values, then reshape to (batch, heads, seq, head_dim)
    k = rearrange(self.key(x), 'b n (h e) -> b h n e', h=self.num_head)
    q = rearrange(self.query(x), 'b n (h e) -> b h n e', h=self.num_head)
    v = rearrange(self.value(x), 'b n (h e) -> b h n e', h=self.num_head)

    # Compute scaled dot-product attention scores
    scale = q.size(-1) ** 0.5
    wei = q @ k.transpose(-2, -1) / scale

    # Apply softmax to get attention weights
    wei = torch.softmax(wei, dim=2)

    # Apply dropout to attention weights
    wei = self.att_dr(wei)

    # Multiply attention weights with values to get attention output
    out = wei @ v

    # Rearrange back to (batch, seq, emb_size)
    out = rearrange(out, 'b h n e -> b n (h e)')
    return out

class FeedForward(nn.Module):
  def __init__(self, emb_size):
    super().__init__()

    # Two-layer feedforward network with hidden size 4x larger than emb_size
    self.ff = nn.Sequential(
        nn.Linear(emb_size, 4 * emb_size),  # Project up
        nn.Linear(4 * emb_size, emb_size)   # Project back down
    )

  def forward(self, x):
    return self.ff(x)

class encoder_block(nn.Module):
  def __init__(self, emb_size, num_head):
    super(encoder_block, self).__init__()

    # Multi-head self-attention block
    self.att = multihead(emb_size, num_head)

    # Layer normalization before attention and feedforward
    self.ll = nn.LayerNorm(emb_size)

    # Dropout to prevent overfitting
    self.dropout = nn.Dropout(0.1)

    # Feedforward network
    self.ff = FeedForward(emb_size)

  def forward(self, x):
    # Apply layer norm -> attention -> dropout
    x = self.dropout(self.att(self.ll(x)))

    # Apply layer norm -> feedforward -> dropout
    x = self.dropout(self.ff(self.ll(x)))
    return x

class VisionTransformer(nn.Module):
  def __init__(self, num_layers, emb_size, patch_size, num_head, num_class, image_size, channels_in=1):
    super(VisionTransformer, self).__init__()

    # Compute number of patches in 3D image (D x H x W), assuming cube shape
    num_patches = (image_size // patch_size) ** 3

    # Patch embedding: split and project image into patch embeddings
    self.patchemb = PatchEmbeddings(channels_in=channels_in, patch_size=patch_size, emb_size=emb_size)

    # Positional embedding to retain spatial information
    self.pos_emb = PositionalEmbedding(seq_len=num_patches + 1, emb_size=emb_size)

    # Stack multiple transformer encoder blocks
    self.attention = nn.Sequential(*[encoder_block(emb_size, num_head) for _ in range(num_layers)])

    # Learnable [CLS] token to summarize the image
    self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))

    # Final classification layer
    self.ff = nn.Linear(emb_size, num_class)

  def forward(self, x):
    # Convert input image to patch embeddings
    x = self.patchemb(x)

    # Expand the [CLS] token to batch size and prepend to patch sequence
    cls_tokens = self.cls_token.expand(x.size(0), -1, -1)
    x = torch.cat((cls_tokens, x), dim=1)

    # Add positional embeddings
    x = self.pos_emb(x)

    # Pass through transformer encoder blocks
    x = self.attention(x)

    # Use the [CLS] token output (first token) for classification
    x = self.ff(x[:, 0, :])
    return x

# Dice Loss Function
class DiceLoss(nn.Module):
  def __init__(self,smooth = 1e-5):
    '''Args:
          smooth - For numerical stability preventing diviosion by zero error
    '''
    super().__init__()
    self.smooth = smooth

  def forward(self, logits, masks):
    '''Args:
          logits - The raw outputs from the model
          masks - Target Masks
       returns:
          A scalar values representing the overlap error between predictions and the masks
    '''

    # logits are converted into probablities and masks are one hot encoded
    masks = masks.squeeze(1)
    probablities = F.softmax(logits, dim = 1)
    masks = F.one_hot(masks, num_classes=3).permute(0,4,1,2,3).float()

    # Calculating the overlap between predictions and ground truth
    intersection = (probablities * masks).sum(dim=(2,3,4))

    # Calulcating the total area of ground truth + predicted masks
    union = probablities.sum(dim=(2,3,4)) +  masks.sum(dim = (2,3,4))

    # Calculating the Dice Coefficent and diceloss
    dice = (2 * intersection + self.smooth)/(union + self.smooth)
    dice_loss = 1 - dice.mean()

    return dice_loss


class multi_fn_loss(nn.Module):
  def __init__(self, dice_we, ce_we):
    '''A loss function that combines DiceLoss and CrossEntropyLoss for accurate segmentation
       Args:
          Dice_we - weight for DiceLoss
          ce_we - Weight for CrossEntropyLoss
    '''
    super().__init__()
    self.dice_we = dice_we
    self.ce_we = ce_we
    self.DiceLoss = DiceLoss()
    self.ce_loss = nn.CrossEntropyLoss()

  def forward(self, logits, masks):
    '''Args:
          Logits - Raw outputs from the model
          masks - Ground truth masks
       returns:
          Dice coefficient
    '''
    return (self.dice_we * self.DiceLoss(logits, masks)) + (self.ce_we * self.ce_loss(logits, masks))

# Creating a dataset with all the CT scans and masks from the training data
images_dir = '/content/drive/MyDrive/AMLIH/AMLH_Medical_image/imagesTrain'
images_paths = sorted(glob.glob(os.path.join(images_dir, 'hippocampus_*.nii.gz')))
images_paths
mask_paths = []
for path in images_paths:
  matches = re.findall(r"\d{3}", path)
  mask_path = '/content/drive/MyDrive/AMLIH/AMLH_Medical_image/labelsTrain'
  mask_path = os.path.join(mask_path, f'hippocampus_{matches[0]}.nii.gz')
  mask_paths.append(mask_path)

# Creating a dataset with all the CT scans and masks from the training data
subjects = []
for (image_path, mask_path) in zip(images_paths, mask_paths):
  subject = tio.Subject(
      CT = tio.ScalarImage(image_path),
      mask = tio.LabelMap(mask_path),
  )
  subjects.append(subject)
dataset = tio.SubjectsDataset(subjects)

# Creating a dataset with all the CT scans and masks from the test data
test_img_dir = '/content/drive/MyDrive/AMLIH/AMLH_Medical_image/imagesTest'
test_img_paths = sorted(glob.glob(os.path.join(test_img_dir, 'hippocampus_*.nii.gz')))
test_mask_paths = []
for path in test_img_paths:
  matches = re.findall(r"\d{3}", path)
  mask_path = '/content/drive/MyDrive/AMLIH/AMLH_Medical_image/labelsTest'
  mask_path = os.path.join(mask_path, f'hippocampus_{matches[0]}.nii.gz')
  test_mask_paths.append(mask_path)

# Creating a dataset with all the CT scans and masks from the test data
test_subjects = []
for (image_path, mask_path) in zip(test_img_paths, test_mask_paths):
  subject = tio.Subject(
      CT = tio.ScalarImage(image_path),
      mask = tio.LabelMap(mask_path),
  )
  test_subjects.append(subject)
test_dataset = tio.SubjectsDataset(subjects)

# Defining transforations for training, validation and test data

training_transform = tio.Compose([
    tio.ToCanonical(),
    tio.Resample(1),
    tio.Resize((32, 32, 32)),
    tio.ZNormalization(masking_method=tio.ZNormalization.mean),

    # Add synthetic bias field to simulate intensity inhomogeneity
    tio.RandomBiasField(coefficients=0.5, p=0.3),

    # Add elastic deformation for realistic anatomical variability
    tio.RandomElasticDeformation(num_control_points=7, max_displacement=7.5, locked_borders=2, p=0.3),

    # Keep existing augmentations
    tio.RandomNoise(p=0.5),
    tio.RandomFlip(axes=(0, 1, 2), flip_probability=0.5),

    #Add gamma transformation to simulate lighting variation
    tio.RandomGamma(log_gamma=(-0.3, 0.3), p=0.3),

    #Add affine transformation (scaling, rotation, translation)
    tio.RandomAffine(scales=(0.9, 1.1), degrees=10, translation=5, p=0.3),
])

validation_transform = tio.Compose([
    tio.ToCanonical(),
    tio.Resample(4),
    tio.Resize((32, 32, 32)),
    tio.ZNormalization(masking_method=tio.ZNormalization.mean),
])


test_transform = tio.Compose([
    tio.ToCanonical(),
    tio.Resample(4),
    tio.Resize((32, 32, 32)),
    tio.ZNormalization(masking_method=tio.ZNormalization.mean),
])

# Saving dataset in all_subjects variable
all_subjects = dataset

# Defining size of training and validation datasets
num_subjects = len(all_subjects)
num_train = int(num_subjects * 0.8)
num_val = num_subjects - num_train

# Splitting the subjects into training and validation subjects
train_subjects, val_subjects = torch.utils.data.random_split(all_subjects, [num_train, num_val])

# Creating training, validaton and test datasets from train_subjects, val_subjects and test_dataset
train_set = tio.SubjectsDataset(list(train_subjects), transform=training_transform)
validation_set = tio.SubjectsDataset(list(val_subjects), transform=validation_transform)
test_set = tio.SubjectsDataset(test_dataset, transform=test_transform)

# Defining train, validation and test dataloaders
train_loader = SubjectsLoader(train_set, batch_size= 8, shuffle=True)
val_loader = SubjectsLoader(validation_set, batch_size=8, shuffle=True)
test_loader = SubjectsLoader(test_set, batch_size=8, shuffle=True)

def calculate_accuracy(preds, masks):
  ''' A function for calulcating the accuracy of a a model predictions by comparing the models output to the masks voxel by voxel
      Args:
          preds - Model outputs (logits) that contain the masks predictions
          masks - Ground truth masks
      return:
          The proportion of correctly classifed voxels compared to the total number of voxels
  '''
  predicted_classes = torch.argmax(preds, dim = 1)
  correct = (predicted_classes == masks).float().sum()
  total = torch.numel(masks)
  return correct/ total


def Dice_coef(preds, masks, smooth = 1e-5):
    '''Function for calculating the Dice coefficient metric
       Args:
          preds - Model outputs (logits) that contain the masks predictions
          masks - Ground truth masks
       returns:
          Dice coefficient metric
    '''

    # Converting the logits into probabilities and one hot encoding the masks
    probablities = F.softmax(preds, dim = 1)
    masks = F.one_hot(masks, num_classes=3).permute(0,4,1,2,3).float()

    # Calculating the intersecton between predictions and Ground truth
    intersection = (probablities * masks).sum(dim=(2,3,4))

    # Calculating total number of voxels in prediction and ground truth
    union = probablities.sum(dim=(2,3,4)) + masks.sum(dim = (2,3,4))

    # Calculating Dice coefficient metric
    dice = (2 * intersection + smooth)/(union + smooth)

    return dice.mean()

def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=15):
    ''' A function that sets up training and validation loops for the model
        Args:
            Model - Untrained model that will output raw logits
            Train_loader - SubjectsLoader that will load the training data into the training loop
            Val_loader - SubjectsLoader that will load validation data into the validation loop
            Criterion - The loss function that will be used adjust weights
            optimizer - The optimizer that will be used to adjust weights
            Device (Torch.device) - if the model will run on the GPU or CPU
            Num_epochs = How much epochs the training and validation loops will run for
        return:
            The model with the updated parameters and a dictionaly with model evaluation metrics
    '''
    model.to(device)

    # Scheduler to reduce learning rate if validation loss plateaus
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)

    # List to store model evaluation metrics
    final_train_loss = []
    final_val_loss = []
    final_train_accuracy = []
    final_val_accuracy = []
    final_train_dice = []
    final_val_dice = []

    for epoch in range(num_epochs):

        # List to store epoch evaluation metrics
        model.train()
        epoch_train_loss = []
        epoch_train_accuracy = []
        epoch_dice_score = []

        # Training loop
        for batch in train_loader:
            # Getting images and masks from train loader
            images = batch['CT']['data'].to(device)
            masks = batch['mask']['data'].to(device).long()
            masks = masks.squeeze(1)

            # Model outputs raw logits used to calulcate loss and evaluation metrics
            outputs = model(images)
            loss = criterion(outputs, masks)
            acc = calculate_accuracy(outputs, masks)
            dice = Dice_coef(outputs, masks)

            # Optimize the weights
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # Appending evaluation metrics to respective lists
            epoch_train_loss.append(loss.item())
            epoch_train_accuracy.append(acc.cpu().item())
            epoch_dice_score.append(dice.cpu().item())

        # Averaging the respective lists
        avg_train_loss = sum(epoch_train_loss) / len(epoch_train_loss)
        avg_train_accuracy = sum(epoch_train_accuracy) / len(epoch_train_accuracy)
        avg_train_dice = sum(epoch_dice_score)/len(epoch_dice_score)

        # Appending the averages to the final lists that hold the scores for each epoch
        final_train_loss.append(avg_train_loss)
        final_train_accuracy.append(avg_train_accuracy)
        final_train_dice.append(avg_train_dice)

        # Setting the model to evaluation mode and creating lists to store validation metrics
        model.eval()
        epoch_val_loss = []
        epoch_val_accuracy = []
        epoch_val_dice = []

        # Setting up validation loops
        with torch.no_grad():
            for batch in val_loader:
                # Loading images and masks
                images = batch['CT']['data'].to(device)
                masks = batch['mask']['data'].to(device).long()
                masks = masks.squeeze(1)

                # Model outputs (logits) used to calulcate loss, accuracy and dice Coefficent
                outputs = model(images)
                loss = criterion(outputs, masks)
                acc = calculate_accuracy(outputs, masks)
                dice = Dice_coef(outputs, masks)

                # Appending evaluation metrics to respective lists
                epoch_val_loss.append(loss.item())
                epoch_val_accuracy.append(acc.cpu().item())
                epoch_val_dice.append(dice.cpu().item())

        # Averaging the respective lists
        avg_val_loss = sum(epoch_val_loss) / len(epoch_val_loss)
        avg_val_accuracy = sum(epoch_val_accuracy) / len(epoch_val_accuracy)
        avg_val_dice = sum(epoch_val_dice) / len(epoch_val_dice)

        # Appending the averages to the final lists that hold the scores for each epoch
        final_val_loss.append(avg_val_loss)
        final_val_accuracy.append(avg_val_accuracy)
        final_val_dice.append(avg_val_dice)

        # Step the learning rate scheduler based on validation loss
        scheduler.step(avg_val_loss)

        # Printing the training and validation loss, accuracy and Dice score for each epoch
        print(f"Epoch [{epoch+1}/{num_epochs}]")
        print(f"  Train     - Loss: {avg_train_loss:.2f}, Accuracy: {avg_train_accuracy:.2f}, Dice: {avg_train_dice:.2f}")
        print(f"  Validation- Loss: {avg_val_loss:.2f}, Accuracy: {avg_val_accuracy:.2f}, Dice: {avg_val_dice:.2f}")

    # Returning a dictionary conatining lists with the evaluation metrics for each epoch
    return {
        'train_loss': final_train_loss,
        'val_loss': final_val_loss,
        'train_accuracy': final_train_accuracy,
        'val_accuracy': final_val_accuracy,
        'Train_dice': final_train_dice,
        'val_dice': final_val_dice
    }

# Setting the device, crierion, class weights and model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
criterion = multi_fn_loss(0.7, 0.3)
Unet_model = Basic_Unet_3D(1,32)
attention_unet = Attention_3D_unet(1,32)
full_attention_unet = full_Attention_3D_unet(1,32)
attention_optimiser = torch.optim.Adam(attention_unet.parameters(), lr=1e-3)
basic_optimiser = torch.optim.Adam(Unet_model.parameters(), lr=1e-3)
full_attention_optimiser = torch.optim.Adam(full_attention_unet.parameters(), lr=1e-3)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

full_attention_metrics = train_model(full_attention_unet, train_loader, val_loader, criterion, full_attention_optimiser, device, 25)

# Training the Unet model
unet_metrics = train_model(Unet_model, train_loader, val_loader, criterion, basic_optimiser, device, 25)

Attention_metrics =  train_model(attention_unet, train_loader, val_loader, criterion, attention_optimiser, device, 25)

def plot_accuracy(train_acc, val_acc):
  ''' A function that plots model accuracy across training and validation epochs
      Args:
        train_acc - List with the model accuracy during training
        val_acc - list with the model accuracy during validation
      returns:
        A line plot of training and validation accuracy across epochs
  '''
  epochs = list(range(1,len(train_acc) + 1))
  plt.plot(epochs, train_acc, label = 'training accuracy')
  plt.plot(epochs, val_acc, label = 'training accuracy')
  plt.xlabel('Epochs')
  plt.ylabel('accuracy')
  plt.legend()
  plt.title('Training and valdaion accuracy over epochs')
  plt.tight_layout()
  plt.show()

def plot_loss(train_loss, val_loss):
  ''' A function that plots model loss across training and validation epochs
      Args:
        train_loss - List with the model loss during training
        val_loss - list with the model loss during validation
      returns:
        A line plot of training and validation loss across epochs
  '''
  epochs = list(range(1, len(train_loss) + 1))
  plt.plot(epochs, train_loss, label = 'Training loss')
  plt.plot(epochs, val_loss, label = 'Val accuracy')
  plt.xlabel('Epochs')
  plt.ylabel('loss')
  plt.legend()
  plt.title('training and validation loss over epochs')
  plt.tight_layout()
  plt.show()

def plot_dice(train_dice, val_dice):
    ''' A function that plots the Dice coefficent scores across training and validation epochs
      Args:
        train_acc - List with the Dice coefficent scores during training
        val_acc - list with the Dice coefficent scores during validation
      returns:
        A line plot of training and validation Dice coefficent scores across epochs
  '''
    epochs = list(range(1, len(train_dice) + 1))
    plt.plot(epochs, train_dice, label = 'training dice score')
    plt.plot(epochs, val_dice, label = 'Val dice score')
    plt.xlabel('epochs')
    plt.ylabel('Dice score')
    plt.legend()
    plt.title('Dice score over epochs')
    plt.tight_layout()
    plt.show()

plot_accuracy(full_attention_metrics['train_accuracy'], full_attention_metrics['val_accuracy'])
plot_loss(full_attention_metrics['train_loss'], full_attention_metrics['val_loss'])
plot_dice(full_attention_metrics['Train_dice'], full_attention_metrics['val_dice'])

plot_accuracy(Attention_metrics['train_accuracy'], Attention_metrics['val_accuracy'])
plot_loss(Attention_metrics['train_loss'], Attention_metrics['val_loss'])
plot_dice(Attention_metrics['Train_dice'], Attention_metrics['val_dice'])

plot_dice(unet_metrics['Train_dice'], unet_metrics['val_dice'])

plot_accuracy(unet_metrics['train_accuracy'], unet_metrics['val_accuracy'])

plot_loss(unet_metrics['train_loss'], unet_metrics['val_loss'])

def test_model(model,test_loader, criterion, device):
  ''' A function to test the model with the test dataset
      Args:
          model - Trained model
          test_loader - Dataloader that will load in the test images
          criertion - The loss function used to calculate loss
          device - Data is loaded onto the CPU or GPU (Torch.device)
      return:
          Model class name and accuracy, loss and Dice coefficent evaluation metrics
  '''

  # sets model to device and creates list that will store evaluation metrics
  model.to(device)
  accuracy = []
  loss_list = []
  dice_list = []

  # Loaded images and masks from the test dataset
  for batch in test_loader:
    images = batch['CT']['data'].to(device)
    masks = batch['mask']['data'].to(device).long()
    masks = masks.squeeze(1)

    # Calulcating loss, accuracy and dice coeiffcent
    outputs = model(images)
    loss = criterion(outputs, masks)
    acc = calculate_accuracy(outputs, masks)
    dice = Dice_coef(outputs, masks)

    # Appending evalulation metrics to there respective lists
    loss_list.append(loss.item())
    accuracy.append(acc.cpu().item())
    dice_list.append(dice.cpu().item())

  # Averging the list to get the final score for each metric
  avg_accuracy = sum(accuracy)/len(accuracy)
  avg_loss = sum(loss_list)/len(loss_list)
  avg_dice = sum(dice_list)/len(dice_list)

  # Printing the class of the model and evaluation metrics
  print(f'Testing: {type(model).__name__}')
  print(f'Accuracy: {avg_accuracy:.2f}, Loss: {avg_loss:.2f}, Dice: {avg_dice:.2f}')

test_model(Unet_model, test_loader, criterion, device)

test_model(attention_unet, test_loader, criterion, device)

test_model(full_attention_unet, test_loader, criterion, device)

# Setting the directory that will store the saved model
save_dir = 'drive/MyDrive/AMLIH/Models'

# Creating the path to store the model
model_path = os.path.join(save_dir, 'Unet_model_2.pth')
torch.save(Unet_model.state_dict(), model_path)

model_path = os.path.join(save_dir, 'Attention_model_2.pth')
torch.save(attention_unet.state_dict(), model_path)

model_path = os.path.join(save_dir, 'Full_Attention_model.pth')
torch.save(full_attention_unet.state_dict(), model_path)

# Loading the unet model
unet_model = Basic_Unet_3D(1,32)
unet_model.load_state_dict(torch.load('drive/MyDrive/AMLIH/Models/Unet_model_2.pth', map_location= torch.device('cpu')))

attention_unet = Attention_3D_unet(1,32)
attention_unet.load_state_dict(torch.load('drive/MyDrive/AMLIH/Models/Attention_model_2.pth', map_location= torch.device('cpu')))

full_attention_unet = full_Attention_3D_unet(1,32)
full_attention_unet.load_state_dict(torch.load('drive/MyDrive/AMLIH/Models/Full_Attention_model.pth', map_location= torch.device('cpu')))

def hausdorff95(pred_mask, true_mask):
    """
    Compute the 95th percentile Hausdorff Distance (HD95).
    pred_mask, true_mask: binary numpy arrays (3D)
    """
    pred_points = np.argwhere(pred_mask)
    true_points = np.argwhere(true_mask)

    if len(pred_points) == 0 or len(true_points) == 0:
        return np.nan  # avoid crash if one mask is empty

    # Pairwise distances
    dists_pred_to_true = cdist(pred_points, true_points).min(axis=1)
    dists_true_to_pred = cdist(true_points, pred_points).min(axis=1)

    # 95th percentile distances
    hd95 = max(np.percentile(dists_pred_to_true, 95),
               np.percentile(dists_true_to_pred, 95))
    return hd95


def test_model_metrics_skimage_3d(model, test_loader, device):
    """
    Test a 3D segmentation model (TorchIO format) and compute:
    - Jaccard (IoU)
    - Hausdorff95 distance
    - Volume similarity
    """
    model.to(device)
    model.eval()

    jaccard_list = []
    hausdorff95_list = []
    volume_similarity_list = []

    with torch.no_grad():
        for batch in test_loader:
            images = batch["CT"]["data"].to(device)      # [B, 1, D, H, W]
            masks = batch["mask"]["data"].to(device)     # [B, 1, D, H, W]
            masks = masks.float()

            outputs = model(images)                      # [B, 1, D, H, W]
            probs = torch.sigmoid(outputs)
            preds = (probs > 0.5).float()

            preds_np = preds.cpu().numpy()
            masks_np = masks.cpu().numpy()

            for b in range(preds_np.shape[0]):
                pred_mask = preds_np[b, 0]  # [D, H, W]
                true_mask = masks_np[b, 0]  # [D, H, W]

                # --- Jaccard (IoU) ---
                intersection = np.logical_and(pred_mask, true_mask).sum()
                union = np.logical_or(pred_mask, true_mask).sum()
                jaccard = (intersection + 1e-6) / (union + 1e-6)
                jaccard_list.append(jaccard)

                # --- Hausdorff95 ---
                hd95 = hausdorff95(pred_mask.astype(bool), true_mask.astype(bool))
                hausdorff95_list.append(hd95)

                # --- Volume similarity ---
                vol_pred = pred_mask.sum()
                vol_true = true_mask.sum()
                vol_sim = 1 - abs(vol_pred - vol_true) / (vol_pred + vol_true + 1e-6)
                volume_similarity_list.append(vol_sim)

    # Averages
    avg_jaccard = np.nanmean(jaccard_list)
    avg_hausdorff95 = np.nanmean(hausdorff95_list)
    avg_volsim = np.nanmean(volume_similarity_list)

    print(f'Testing: {type(model).__name__}')
    print(f'Jaccard: {avg_jaccard:.4f}, Hausdorff95: {avg_hausdorff95:.4f}, Volume Similarity: {avg_volsim:.4f}')

test_model_metrics_skimage_3d(full_attention_unet, test_loader, device)

test_model_metrics_skimage_3d(unet_model, test_loader, device)

def test_processing(path, transform, model):
  '''A function for testing the model on a new scan
  Args:
      Path - Path to where the test scan is stored
      transform - test transformation that images will under go before entering the model
      model - Trained model
  returns:
      A combined mask with 2 distinct regions indicating the left and right hypothalamus and unnormalised test image
  '''

  # Loads a medical image and filers out background voxels
  img = tio.ScalarImage(path)
  tensor = img.data
  mask = tensor != 0
  values = tensor[mask]

  # Saves the mean and std from all non zero pixels used for unnormalisation
  mean = values.mean()
  std = values.std()

  # Transforming the image before putting it into the model
  test_img = test_transform(img)
  model.to(torch.device('cpu'))
  with torch.no_grad():
    output = model(test_img.data.unsqueeze(0))

  # Creating the masks for the left and right hypothalamus
  probs = torch.softmax(output, dim=1)
  preds = torch.argmax(probs, dim = 1)
  mask1 = (preds == 1).float()
  mask2 = (preds == 2).float() * 2

  # Unnormalising the test image
  unnorm_img = test_img.data * std + mean

  # returning the combined masks an unormalised test image for visulisation
  return mask1 + mask2, unnorm_img

def reverse_transform(mask, transformed_img, orginal_img):
  ''' A function that reverses the transfornations put on the image
      Args:
          Mask - The combined mask generated from test processing function
          Transformed_img - The transformed img generated from test pre processing function
          orginal_img - The orginal image, extract meta into formation to return transformed image back to the orginal image
      Returns:
          torch.Tensor: Reverse-transformed mask tensor aligned with the original image.
  '''

  # Craeting a labelclass to represent multi-class mask with the same spacial information as the orginal image
  pred_mask = tio.LabelMap(tensor= mask, affine = orginal_img.affine)

  # Defining the reverse transformations
  reverse_tranformation = tio.Compose([
      tio.Resize(orginal_img.shape[1:], image_interpolation='nearest'),
      tio.Resample(orginal_img, image_interpolation='nearest'),
  ])

  # Return the transformed test image
  return reverse_tranformation(pred_mask)

def compare_image(image_num, transform, model, slices, alpha):
  ''' A function that visualises a series of images with the predicted masks compared to the orginal masks
      Args:
          image_num (3 digit string) - what image you want to visualise
          transform - The transformation you want to put on the image
          model
  '''
  image_path = f'drive/MyDrive/AMLIH/AMLH_Medical_image/imagesTrain/hippocampus_{image_num}.nii.gz'
  mask_path = f'drive/MyDrive/AMLIH/AMLH_Medical_image/labelsTrain/hippocampus_{image_num}.nii.gz'
  combined_mask, transformed_img = test_processing(image_path, transform, model)
  orginal_img = tio.ScalarImage(image_path)
  reverse_mask = reverse_transform(combined_mask, transformed_img, orginal_img)
  print('Predicted mask series')
  # Convert reverse_mask to numpy array before passing to image_series
  image_series(orginal_img.data.squeeze().numpy(), reverse_mask.data.squeeze().numpy(), slices, alpha)
  print('Orginal mask series')
  img = nib.load(image_path)
  mask = nib.load(mask_path) # Load the correct mask path
  img = img.get_fdata()
  mask = mask.get_fdata()
  image_series(img, mask, slices, alpha)

compare_image('386', test_transform, attention_unet, 8, 0.5)

compare_image('302', test_transform, attention_unet, 8, 0.5)

compare_image('224', test_transform, attention_unet, 8, 0.5)

compare_image('386', test_transform, unet_model, 8, 0.5)

compare_image('302', test_transform, unet_model, 8, 0.5)

compare_image('224', test_transform, unet_model, 8, 0.5)

compare_image('224', test_transform, full_attention_unet, 8, 0.5)

compare_image('298', test_transform, full_attention_unet, 8, 0.5)

os.listdir('drive/MyDrive/AMLIH/AMLH_Medical_image/labelsTrain')