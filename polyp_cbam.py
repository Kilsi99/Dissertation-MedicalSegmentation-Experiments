# -*- coding: utf-8 -*-
"""polyp_CBAM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uTj9gGcg-X5M7ybmoA5dUy65oOkqArBV
"""

!pip install einops

import os
from google.colab import drive
drive.mount('/content/drive')
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import cv2
from google.colab.patches import cv2_imshow as CV2_imshow
import torch
import torchvision
from torchvision import transforms
import torch.nn as nn
import torch.nn.functional as F
import re
from torch.utils.data import Dataset, DataLoader, random_split
from einops import rearrange
from torch.optim.lr_scheduler import ReduceLROnPlateau
from skimage.metrics import hausdorff_distance
from scipy.spatial.distance import cdist

img_dir = '/content/drive/MyDrive/Dissertation/polyp_datasets/CVC-ClinicDB/PNG/Original'
mask_dir = '/content/drive/MyDrive/Dissertation/polyp_datasets/CVC-ClinicDB/PNG/Ground Truth'
os.listdir(mask_dir)[1]

case_list = [re.findall('\d{1,3}', x)[0] for x in os.listdir(img_dir)]

def load_img_mask(case_num):

  img_path = os.path.join(img_dir, f'{case_num}.png')
  mask_path = os.path.join(mask_dir, f'{case_num}.png')

  img =  Image.open(img_path).convert('RGB')
  mask =  Image.open(mask_path).convert('L')

  return np.array(img), np.array(mask)

img, mask = load_img_mask(case_list[50])

def draw_mask(img, mask, scale):
  masked_image = img.copy()
  mask = np.expand_dims(mask,axis= 2)

  masked_image = np.where(mask.astype(int),
                          np.array([0,255,0], dtype = 'uint8'),
                          masked_image)

  masked_image = masked_image.astype(np.uint8)

  segmented_image = cv2.addWeighted(img, 0.8, masked_image, 0.2, 0)
  height, width = segmented_image.shape[:2]
  new_size = (int(scale * height), int(scale * width))

  resized = cv2.resize(segmented_image, new_size)
  CV2_imshow(resized)

draw_mask(img, mask, 0.5)

class SegmentationDataset(Dataset):
  def __init__(self, image_dir, mask_dir, img_transform = None, mask_transform = None):
    self.image_dir = image_dir
    self.mask_dir = mask_dir
    self.img_transform = img_transform
    self.mask_transform = mask_transform
    self.image_names = os.listdir(image_dir)

  def __len__(self):
    return len(self.image_names)

  def __getitem__(self, idx):
    match = re.findall('\d{1,3}', self.image_names[idx])
    img_path = os.path.join(self.image_dir, f'{match[0]}.png')
    mask_path = os.path.join(self.mask_dir, f'{match[0]}.png')

    img = Image.open(img_path).convert('RGB')
    if self.img_transform:
      img = self.img_transform(img)
    else:
      img = torch.from_numpy(np.array(img)).permute(2, 0, 1).float() / 255.0

    mask = Image.open(mask_path).convert('L')
    if self.mask_transform:
      mask = self.mask_transform(mask)
    else:
      mask = torch.from_numpy(np.array(mask)).long()

    return img, mask

test_set = SegmentationDataset(img_dir,mask_dir, img_transform= None, mask_transform= None)
len(test_set)

image_paths = [os.path.join(img_dir, f'{x}.png') for x in case_list]
image_paths[0]

to_tensor = transforms.ToTensor()

channel_sum = torch.zeros(3)
channel_squared_sum = torch.zeros(3)
num_pixels = 0

for img_path in image_paths:
  img = Image.open(img_path).convert('RGB')
  img_tensor = to_tensor(img)

  c,h,w = img_tensor.shape
  num_pixels += h * w

  channel_sum += img_tensor.sum(dim=[1,2])
  channel_squared_sum += (img_tensor ** 2).sum(dim=[1,2])


# Compute mean and std
mean = channel_sum / num_pixels
std = (channel_squared_sum / num_pixels - mean ** 2).sqrt()

print("Mean per channel:", mean)
print("Std per channel:", std)

img_transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.RandomRotation(degrees=15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.RandomAffine(degrees=0, translate=(0.1,0.1), scale=(0.9,1.1)),
    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.4008, 0.2694, 0.1841],
                         std=[0.2984, 0.2050, 0.1397])
])

mask_transform = transforms.Compose([
    transforms.Resize((256, 256), interpolation=Image.NEAREST),
    transforms.ToTensor(),
])

test_img_transform = transforms.Compose([
    transforms.Resize((256,256)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.4008, 0.2694, 0.1841],
                         std=[0.2984, 0.2050, 0.1397])
])


full_dataset = SegmentationDataset(img_dir, mask_dir, img_transform, mask_transform)

train_len = int(len(full_dataset) * 0.7)
val_len = int((len(full_dataset) - train_len)/2)
test_len = val_len

print(train_len, val_len)

generator = torch.Generator().manual_seed(100)
train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_len, val_len, test_len], generator)


train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=True)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, drop_last=True)
test_loader = DataLoader(test_dataset, batch_size = 8, shuffle= False)

class channel_attention(nn.Module):
  def __init__(self, channels_in, reduction_ratio = 16):
    super(channel_attention, self).__init__()

    self.mlp = nn.Sequential(
        nn.Linear(channels_in, channels_in//reduction_ratio, bias = False),
        nn.ReLU(inplace = True),
        nn.Linear(channels_in// reduction_ratio, channels_in, bias = False)
    )
    self.avg_pool = nn.AdaptiveAvgPool2d(1)
    self.max_pool = nn.AdaptiveMaxPool2d(1)
    self.sigmoid = nn.Sigmoid()

  def forward(self, x):
    b,c,_,_= x.size()
    avg_out = self.mlp(self.avg_pool(x).view(b,c))
    max_out = self.mlp(self.max_pool(x).view(b,c))
    out = avg_out + max_out
    scale = self.sigmoid(out).view(b,c,1,1)
    return x * scale

class spatial_attention(nn.Module):
  def __init__(self, kernel_size = 7):
    super(spatial_attention, self).__init__()
    padding = (kernel_size -1) // 2

    self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)
    self.sigmoid = nn.Sigmoid()

  def forward(self, x):

    avg_out = torch.mean(x, dim=1, keepdim= True)
    max_out, _ = torch.max(x, dim = 1, keepdim= True)

    x_cat = torch.cat([avg_out, max_out], dim=1)

    attention = self.sigmoid(self.conv(x_cat))

    return x * attention

class CBAM(nn.Module):
  def __init__(self, channels_in, reduction_ratio = 16, kernel_size =7):
    super(CBAM, self).__init__()

    self.channel_attention = channel_attention(channels_in, reduction_ratio)
    self.spatial_attention = spatial_attention(kernel_size)

  def forward(self, x):
    x = self.channel_attention(x)
    x = self.spatial_attention(x)
    return x

# Creating a 2D unet Architecture with attention gates
class Encoder_block(nn.Module):
  def __init__(self, channels_in, channels_out):
    '''Args:
          channels_in - Number of channels inputed into the model or pevious encoder block.
          1 for greyscale CT scans (model input)
          Channels_out - Base numbers of feature maps that will be upscaled throughut the encder or
          passed to the decoder if its the final encoder block

    '''
    super(Encoder_block, self).__init__()

    # the encoder compresses spatial dimentions while increasing feature depth
    # Each encoder block consist of 2x (2D convolution and Relu activation) followed by Max pooling
    # Max pooling half spacial demintions - D/H/W
    self.conv = nn.Sequential(
        nn.Conv2d(channels_in, channels_out, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU(inplace = True),
        nn.Conv2d(channels_out, channels_out, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU(inplace = True)
        )
    self.pool = nn.MaxPool2d(2,2)
    self.CBAM = CBAM(channels_out)

  # Defining how data will move through the encoder
  def forward(self, x):
    '''Args:
          x (torch.Tensor) - Input Tensor shape [B, Channels_in, D, H, W]
          Returns - x: Channels_out number of feature maps that will go to decoder
                       via the skip connection
                    p: Downsampled Feature maps after pooling. These will travel down the encoder
    '''
    x = self.conv(x)
    x = self.CBAM(x)
    p = self.pool(x)
    return x , p

class attention_gate(nn.Module):
  def __init__(self, channels_in, skip_channels, channels_out):
    super(attention_gate, self).__init__()

    self.Wg = nn.Sequential(
        nn.Conv2d(channels_in, channels_out, kernel_size = 3, stride = 1, padding = 1),
        nn.BatchNorm2d(channels_out)
    )

    self.Ws = nn.Sequential(
        nn.Conv2d(skip_channels, channels_out, kernel_size = 3, stride = 1, padding = 1),
        nn.BatchNorm2d(channels_out)
    )

    self.ReLU = nn.ReLU(inplace = True)
    self.output = nn.Sequential(
        nn.Conv2d(channels_out, channels_out, kernel_size = 3, stride = 1, padding = 1),
        nn.Sigmoid()
    )

  def forward(self, x, skip):
    wg = self.Wg(x)
    ws = self.Ws(skip)
    out = self.ReLU(wg + ws)
    out = self.output(out)
    return out * skip

# Defining the decoder block - Upsamples back to the orginal size using ConvTranspose2D
# Each stage halves feature depth ad doubles spacial information
class Decoder_block(nn.Module):
  def __init__(self, channels_in, skip_channels, channels_out):
    '''Args:
          channels_in: Input feature maps from the bottleneck or previous decoder block
          Skip_channels: feature maps from the encoder coming through the skip connection
          channels_out: feature maps outputted to the next decder block or final output layer

    '''
    super(Decoder_block, self).__init__()

    self.upsample = nn.ConvTranspose2d(channels_in, channels_out, kernel_size=2, stride = 2)
    self.attention_gate = attention_gate(channels_out, skip_channels, channels_out)
    self.conv = nn.Sequential(
         nn.Conv2d(channels_out + skip_channels, channels_out, kernel_size=3, stride = 1, padding = 1),
         nn.ReLU(inplace = True),
         nn.Conv2d(channels_out, channels_out, kernel_size=3, stride = 1, padding = 1),
         nn.ReLU(inplace = True)
    )
    self.CBAM = CBAM(channels_out)

  def forward(self, x, skip):
    '''Args:
          x - Features maps from bottleneck or previous decoder block
          Skip - Features maps coming from the encoder via the skip connection
       returns:
          Upsampled feature maps the will be used in the next decoder block or ouput layer
    '''
    x = self.upsample(x)
    gated_skip = self.attention_gate(x, skip)
    x = torch.cat([x, gated_skip], dim=1)
    x = self.conv(x)
    x = self.CBAM(x)
    return x


# Defining 2D Unet architecture - Data flows through 4 encoder blocoks reducing spacial size (D/H/W)
# Skip connections send feature maps to decoder by max pooling in each block
# Decoder upsamples back to orginal image size, feature maps are downscaled
class full_Attention_unet(nn.Module):
  def __init__(self, channels_in, channels_out):
    '''Args:
          channels_in - num channels inputted into first encoder block, 1 for greyscale CT scan
          Channels_out - Inital number of feature maps that will be outputted by first encoder block.
          This will be upscaled during the encoder and and downscaled during the decoder.
    '''
    super(full_Attention_unet, self).__init__()

    # 4 encoder blocks - upscaled feature maps and reduces spacial deimentions of image
    # First 3 send features maps to corresponding decoder block, last sends feature maps to the bottlebeck
    self.Encoder_block1 = Encoder_block(channels_in, channels_out)
    self.Encoder_block2 = Encoder_block(channels_out, channels_out * 2)
    self.Encoder_block3 = Encoder_block(channels_out * 2, channels_out * 4)
    self.Encoder_block4 = Encoder_block(channels_out * 4, channels_out * 8)

    # Bottleneck learns compressed high-level features at the lowest resolution between encoder and decoder.
    self.bottle_neck = nn.Sequential(
        nn.Conv2d(channels_out * 8, channels_out * 16, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU(inplace = True),
        nn.Conv2d(channels_out * 16, channels_out * 16, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU()
    )

    # Decoder architecure unsamples image spacial dimentions and reduces feature maps number
    # 1st decoder block inputs feature maps from bottleneck, next 3 lays recieves feature maps
    # from previous decoder block. Feature then fed to output layer
    self.decoder_block4 = Decoder_block(channels_out * 16, channels_out * 8, channels_out * 8)
    self.decoder_block3 = Decoder_block(channels_out * 8, channels_out * 4, channels_out * 4)
    self.decoder_block2 = Decoder_block(channels_out * 4, channels_out * 2, channels_out * 2)
    self.decoder_block1 = Decoder_block(channels_out * 2, channels_out , channels_out)

    # Map decoder output to 3 output chanbels, blackground left hypothalamus and right hypothalamus
    self.output = nn.Conv2d(channels_out, 2, kernel_size=3, stride = 1, padding = 1)


  def forward(self, x):
    '''Args:
          x (Torch.Tensor) - input tensor of shape [B, channels_in, D,H,W]
       returns:
          Torch.Tensor: Segmentation logits of shape [Batch, 3, Depth, Height, widith]
                         3-class output - Background, left hypothalamus, right hypothalamus
    '''
    x1, p1 = self.Encoder_block1(x)
    x2, p2 = self.Encoder_block2(p1)
    x3, p3 = self.Encoder_block3(p2)
    x4, p4 = self.Encoder_block4(p3)

    x5 = self.bottle_neck(p4)

    d4 = self.decoder_block4(x5, x4)
    d3 = self.decoder_block3(d4, x3)
    d2 = self.decoder_block2(d3, x2)
    d1 = self.decoder_block1(d2, x1)

    output = self.output(d1)

    return output

class DiceLoss(nn.Module):
    def __init__(self, smooth=1e-5):
        super().__init__()
        self.smooth = smooth

    def forward(self, logits, masks):
        # Squeeze channel dimension if present
        masks = masks.squeeze(1)  # [B, H, W]
        probabilities = F.softmax(logits, dim=1)  # [B, C, H, W]
        masks = F.one_hot(masks.long(), num_classes=logits.shape[1])  # [B, H, W, C]
        masks = masks.permute(0, 3, 1, 2).float()  # [B, C, H, W]

        intersection = (probabilities * masks).sum(dim=(2,3))
        union = probabilities.sum(dim=(2,3)) + masks.sum(dim=(2,3))
        dice = (2 * intersection + self.smooth) / (union + self.smooth)
        dice_loss = 1 - dice.mean()

        return dice_loss


class multi_fn_loss(nn.Module):
  def __init__(self, dice_we, ce_we):
    '''A loss function that combines DiceLoss and CrossEntropyLoss for accurate segmentation
       Args:
          Dice_we - weight for DiceLoss
          ce_we - Weight for CrossEntropyLoss
    '''
    super().__init__()
    self.dice_we = dice_we
    self.ce_we = ce_we
    self.DiceLoss = DiceLoss()
    self.ce_loss = nn.CrossEntropyLoss()

  def forward(self, logits, masks):
    '''Args:
          Logits - Raw outputs from the model
          masks - Ground truth masks
       returns:
          Dice coefficient
    '''
    return (self.dice_we * self.DiceLoss(logits, masks)) + (self.ce_we * self.ce_loss(logits, masks))

def calculate_accuracy(preds, masks):
  ''' A function for calulcating the accuracy of a a model predictions by comparing the models output to the masks voxel by voxel
      Args:
          preds - Model outputs (logits) that contain the masks predictions
          masks - Ground truth masks
      return:
          The proportion of correctly classifed voxels compared to the total number of voxels
  '''
  predicted_classes = torch.argmax(preds, dim = 1)
  correct = (predicted_classes == masks).float().sum()
  total = torch.numel(masks)
  return correct/ total


def Dice_coef(preds, masks, smooth = 1e-5):
    '''Function for calculating the Dice coefficient metric
       Args:
          preds - Model outputs (logits) that contain the masks predictions
          masks - Ground truth masks
       returns:
          Dice coefficient metric
    '''

    # Converting the logits into probabilities and one hot encoding the masks
    probablities = F.softmax(preds, dim = 1)
    masks = F.one_hot(masks, num_classes=2).permute(0,3,1,2).float()

    # Calculating the intersecton between predictions and Ground truth
    intersection = (probablities * masks).sum(dim=(2,3))

    # Calculating total number of voxels in prediction and ground truth
    union = probablities.sum(dim=(2,3)) + masks.sum(dim=(2,3))

    # Calculating Dice coefficient metric
    dice = (2 * intersection + smooth)/(union + smooth)

    return dice.mean()

def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=15):
    ''' A function that sets up training and validation loops for the model
        Args:
            Model - Untrained model that will output raw logits
            Train_loader - SubjectsLoader that will load the training data into the training loop
            Val_loader - SubjectsLoader that will load validation data into the validation loop
            Criterion - The loss function that will be used adjust weights
            optimizer - The optimizer that will be used to adjust weights
            Device (Torch.device) - if the model will run on the GPU or CPU
            Num_epochs = How much epochs the training and validation loops will run for
        return:
            The model with the updated parameters and a dictionaly with model evaluation metrics
    '''
    model.to(device)

    # Scheduler to reduce learning rate if validation loss plateaus
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)

    # List to store model evaluation metrics
    final_train_loss = []
    final_val_loss = []
    final_train_accuracy = []
    final_val_accuracy = []
    final_train_dice = []
    final_val_dice = []

    for epoch in range(num_epochs):

        # List to store epoch evaluation metrics
        model.train()
        epoch_train_loss = []
        epoch_train_accuracy = []
        epoch_dice_score = []

        # Training loop
        for img, mask in train_loader:
            # Getting images and masks from train loader
            img, masks = img.to(device), mask.to(device)
            masks = masks.squeeze(1).long()

            # Model outputs raw logits used to calulcate loss and evaluation metrics
            outputs = model(img)
            loss = criterion(outputs, masks)
            acc = calculate_accuracy(outputs, masks)
            dice = Dice_coef(outputs, masks)

            # Optimize the weights
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # Appending evaluation metrics to respective lists
            epoch_train_loss.append(loss.item())
            epoch_train_accuracy.append(acc.cpu().item())
            epoch_dice_score.append(dice.cpu().item())

        # Averaging the respective lists
        avg_train_loss = sum(epoch_train_loss) / len(epoch_train_loss)
        avg_train_accuracy = sum(epoch_train_accuracy) / len(epoch_train_accuracy)
        avg_train_dice = sum(epoch_dice_score)/len(epoch_dice_score)

        # Appending the averages to the final lists that hold the scores for each epoch
        final_train_loss.append(avg_train_loss)
        final_train_accuracy.append(avg_train_accuracy)
        final_train_dice.append(avg_train_dice)

        # Setting the model to evaluation mode and creating lists to store validation metrics
        model.eval()
        epoch_val_loss = []
        epoch_val_accuracy = []
        epoch_val_dice = []

        # Setting up validation loops
        with torch.no_grad():
            for img, masks in val_loader:
                # Loading images and masks
                img, masks = img.to(device), mask.to(device)
                masks = masks.squeeze(1).long()

                # Model outputs (logits) used to calulcate loss, accuracy and dice Coefficent
                outputs = model(img)
                loss = criterion(outputs, masks)
                acc = calculate_accuracy(outputs, masks)
                dice = Dice_coef(outputs, masks)

                # Appending evaluation metrics to respective lists
                epoch_val_loss.append(loss.item())
                epoch_val_accuracy.append(acc.cpu().item())
                epoch_val_dice.append(dice.cpu().item())

        # Averaging the respective lists
        avg_val_loss = sum(epoch_val_loss) / len(epoch_val_loss)
        avg_val_accuracy = sum(epoch_val_accuracy) / len(epoch_val_accuracy)
        avg_val_dice = sum(epoch_val_dice) / len(epoch_val_dice)

        # Appending the averages to the final lists that hold the scores for each epoch
        final_val_loss.append(avg_val_loss)
        final_val_accuracy.append(avg_val_accuracy)
        final_val_dice.append(avg_val_dice)

        # Step the learning rate scheduler based on validation loss
        scheduler.step(avg_val_loss)

        # Printing the training and validation loss, accuracy and Dice score for each epoch
        print(f"Epoch [{epoch+1}/{num_epochs}]")
        print(f"  Train     - Loss: {avg_train_loss:.2f}, Accuracy: {avg_train_accuracy:.2f}, Dice: {avg_train_dice:.2f}")
        print(f"  Validation- Loss: {avg_val_loss:.2f}, Accuracy: {avg_val_accuracy:.2f}, Dice: {avg_val_dice:.2f}")

    # Returning a dictionary conatining lists with the evaluation metrics for each epoch
    return {
        'train_loss': final_train_loss,
        'val_loss': final_val_loss,
        'train_accuracy': final_train_accuracy,
        'val_accuracy': final_val_accuracy,
        'Train_dice': final_train_dice,
        'val_dice': final_val_dice
    }

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
CBAM_model = full_Attention_unet(3,32)
criterion = multi_fn_loss(0.95, 0.05)
optimiser = torch.optim.Adam(CBAM_model.parameters(), lr=1e-4)

CBAM_metrics = train_model(CBAM_model, train_loader, val_loader, criterion, optimiser, device, 25)

def plot_accuracy(train_acc, val_acc, title='Training and validation loss over epochs: CBAM_unet'):
    ''' A function that plots model accuracy across training and validation epochs
        Args:
          train_acc - List with the model accuracy during training
          val_acc - list with the model accuracy during validation
          title - Optional string for plot title
        returns:
          A line plot of training and validation accuracy across epochs
    '''
    epochs = list(range(1, len(train_acc) + 1))
    plt.plot(epochs, train_acc, label='Training accuracy')
    plt.plot(epochs, val_acc, label='Validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend(loc='center right', bbox_to_anchor=(1, 0.7))
    plt.title(title if title else 'Training and validation accuracy over epochs')
    plt.tight_layout()
    plt.show()

def plot_loss(train_loss, val_loss, title='Training and validation loss over epochs: CBAM_unet'):
    ''' A function that plots model loss across training and validation epochs
        Args:
          train_loss - List with the model loss during training
          val_loss - list with the model loss during validation
          title - Optional string for plot title
        returns:
          A line plot of training and validation loss across epochs
    '''
    epochs = list(range(1, len(train_loss) + 1))
    plt.plot(epochs, train_loss, label='Training loss')
    plt.plot(epochs, val_loss, label='Validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend(loc='best')
    plt.title(title if title else 'Training and validation loss over epochs')
    plt.tight_layout()
    plt.show()

def plot_dice(train_dice, val_dice, title='Training and validation loss over epochs: CBAM_unet'):
    ''' A function that plots the Dice coefficient scores across training and validation epochs
        Args:
          train_dice - List with the Dice coefficient scores during training
          val_dice - list with the Dice coefficient scores during validation
          title - Optional string for plot title
        returns:
          A line plot of training and validation Dice coefficient scores across epochs
    '''
    epochs = list(range(1, len(train_dice) + 1))
    plt.plot(epochs, train_dice, label='Training Dice score')
    plt.plot(epochs, val_dice, label='Validation Dice score')
    plt.xlabel('Epochs')
    plt.ylabel('Dice score')
    plt.legend(loc='best')
    plt.title(title if title else 'Dice score over epochs')
    plt.tight_layout()
    plt.show()

plot_accuracy(CBAM_metrics['train_accuracy'],CBAM_metrics['val_accuracy'])
plot_loss(CBAM_metrics['train_loss'], CBAM_metrics['val_loss'])
plot_dice(CBAM_metrics['Train_dice'], CBAM_metrics['val_dice'])

def save_model(model, save_dir, filename="CBAM_model.pth"):
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, filename)
    torch.save(model.state_dict(), save_path)
    print(f"Model saved to {save_path}")


save_model(CBAM_model, '/content/drive/MyDrive/Dissertation/polyp_datasets/models')

CBAM_model = full_Attention_unet(3,32)
CBAM_model.load_state_dict(torch.load('/content/drive/MyDrive/Dissertation/polyp_datasets/models/CBAM_model.pth', map_location= torch.device('cpu')))

class Dice_coef(nn.Module):
    def __init__(self, smooth=1e-6):
        super(Dice_coef, self).__init__()
        self.smooth = smooth

    def forward(self, logits, masks):
        # Apply sigmoid to logits
        probabilities = torch.sigmoid(logits)

        # If masks are [B, H, W], unsqueeze to [B, 1, H, W]
        if masks.dim() == 3:
            masks = masks.unsqueeze(1)

        masks = masks.float().to(logits.device)

        # Dice coefficient calculation
        intersection = (probabilities * masks).sum(dim=(2, 3))
        union = probabilities.sum(dim=(2, 3)) + masks.sum(dim=(2, 3))
        dice = (2 * intersection + self.smooth) / (union + self.smooth)

        return dice.mean()


def test_model(model, test_loader, criterion, device):
    model.to(device)
    model.eval()

    dice_fn = Dice_coef().to(device)
    accuracy = []
    loss_list = []
    dice_list = []

    with torch.no_grad():
        for img, masks in test_loader:
            img, masks = img.to(device), masks.to(device)
            # For BCEWithLogitsLoss:
            # masks = masks.float()
            # For CrossEntropyLoss:
            masks = masks.long()  # if you use CrossEntropyLoss
            masks = masks.squeeze(1) # Squeeze the channel dimension

            outputs = model(img)  # (B, num_classes, H, W)
            loss = criterion(outputs, masks)

            acc = calculate_accuracy(outputs, masks)
            dice = dice_fn(outputs, masks)

            loss_list.append(loss.item())
            accuracy.append(acc.cpu().item())
            dice_list.append(dice.cpu().item())

    avg_accuracy = sum(accuracy) / len(accuracy)
    avg_loss = sum(loss_list) / len(loss_list)
    avg_dice = sum(dice_list) / len(dice_list)

    print(f'Testing: {type(model).__name__}')
    print(f'Accuracy: {avg_accuracy:.2f}, Loss: {avg_loss:.2f}, Dice: {avg_dice:.2f}')

test_model(CBAM_model, test_loader, criterion, device)

def test_model_metrics_skimage(model, test_loader, device):
    """
    Test a segmentation model and compute Jaccard (IoU), Hausdorff distance, and Volume Similarity.
    Uses skimage.metrics.hausdorff_distance for Hausdorff.
    """
    model.to(device)
    model.eval()

    jaccard_list = []
    hausdorff_list = []
    volume_similarity_list = []

    with torch.no_grad():
        for img, masks in test_loader:
            img, masks = img.to(device), masks.to(device)
            # Ensure masks shape [B, 1, H, W]
            if masks.dim() == 3:
                masks = masks.unsqueeze(1)
            masks = masks.float()

            outputs = model(img)
            probs = torch.sigmoid(outputs)
            preds = (probs > 0.5).float()

            # Convert to numpy for skimage
            preds_np = preds.cpu().numpy()
            masks_np = masks.cpu().numpy()

            for b in range(preds_np.shape[0]):
                pred_mask = preds_np[b,0]
                true_mask = masks_np[b,0]

                # Jaccard (IoU)
                intersection = np.logical_and(pred_mask, true_mask).sum()
                union = np.logical_or(pred_mask, true_mask).sum()
                jaccard = (intersection + 1e-6) / (union + 1e-6)
                jaccard_list.append(jaccard)

                # Hausdorff distance
                try:
                    hd = hausdorff_distance(pred_mask.astype(bool), true_mask.astype(bool))
                except ValueError:
                    hd = np.nan  # if one mask is empty
                hausdorff_list.append(hd)

                # Volume similarity
                vol_pred = pred_mask.sum()
                vol_true = true_mask.sum()
                vol_sim = 1 - abs(vol_pred - vol_true) / (vol_pred + vol_true + 1e-6)
                volume_similarity_list.append(vol_sim)

    avg_jaccard = np.nanmean(jaccard_list)
    avg_hausdorff = np.nanmean(hausdorff_list)
    avg_volsim = np.nanmean(volume_similarity_list)

    print(f'Testing: {type(model).__name__}')
    print(f'Jaccard: {avg_jaccard:.4f}, Hausdorff: {avg_hausdorff:.4f}, Volume Similarity: {avg_volsim:.4f}')

test_model_metrics_skimage(CBAM_model, test_loader, device)

def predict_and_visualize_mask(case_num, model, device, transform, image_dir, mask_dir):
    """
    Loads an image, predicts the segmentation mask using a trained U-Net model,
    resizes the mask to original image size, and visualizes it.

    Args:
        case_num (str/int): Case number of the image.
        model (torch.nn.Module): Trained segmentation model.
        device (torch.device): Device (CPU or GPU) to run inference.
        transform (torchvision.transforms): Preprocessing transform for the model.
        image_dir (str): Directory containing the images.
        mask_dir (str): Directory containing the masks.
    """
    # Load mask
    mask_path = os.path.join(mask_dir, f'ISIC_{case_num}_Segmentation.png')
    mask =  Image.open(mask_path).convert('L')
    # Load image
    img_path = os.path.join(image_dir, f'ISIC_{case_num}.jpg')
    img = Image.open(img_path).convert('RGB')
    img_np = np.array(img)
    if img_np.dtype != np.uint8:
        img_np = (img_np * 255).astype(np.uint8)

    pil_img = Image.fromarray(img_np)
    original_size = pil_img.size  # (width, height)

    # Prepare tensor
    input_tensor = transform(pil_img).unsqueeze(0).to(device)

    # Predict mask
    model.eval()
    with torch.no_grad():
        output = model(input_tensor)  # shape: (1, 2, H, W)

    output = output.squeeze(0)  # shape: (2, H, W)
    pred_mask = torch.argmax(output, dim=0).cpu().numpy()  # (H, W)

    # Resize to original size
    mask_pil = Image.fromarray(pred_mask.astype(np.uint8), mode='L')
    mask_resized = mask_pil.resize(original_size, resample=Image.NEAREST)
    mask_resized_np = np.array(mask_resized)

    # Display
    plt.figure(figsize=(6,6))
    plt.imshow(mask_resized_np, cmap='gray')
    plt.title(f"Predicted Mask for Case {case_num}")
    plt.axis('off')
    plt.show()

    return mask_resized_np, img, mask

def plot_pred_vs_gt_from_masks(case_list, masks_imgs_list, max_cases=5):
    """
    Plots predicted vs ground-truth mask overlays in subplots for multiple cases.

    Args:
        case_list (list): List of case numbers.
        masks_imgs_list (list of tuples): Each element is (pred_mask, img, gt_mask)
        max_cases (int): Maximum number of cases to display.
    """
    def overlay_mask_on_image(img, mask, color=[255, 0, 0], alpha=0.4):
        """Overlay a binary mask on an image."""
        if isinstance(img, Image.Image):
            img = np.array(img)
        overlay = img.copy()
        mask_bin = (mask > 0).astype(np.uint8)
        color_mask = np.zeros_like(img)
        color_mask[mask_bin == 1] = color
        cv2.addWeighted(color_mask, alpha, overlay, 1.0, 0, overlay)
        return overlay

    num_cases = min(len(case_list), max_cases)
    fig, axes = plt.subplots(num_cases, 2, figsize=(10, 5*num_cases))

    if num_cases == 1:
        axes = np.expand_dims(axes, axis=0)  # Ensure 2D indexing

    for i, case_num in enumerate(case_list[:num_cases]):
        pred_mask, img, gt_mask = masks_imgs_list[i]

        # Overlay masks
        pred_overlay = overlay_mask_on_image(img, pred_mask, color=[255, 0, 0], alpha=0.4)
        gt_overlay = overlay_mask_on_image(img, np.array(gt_mask), color=[0, 255, 0], alpha=0.4)

        # Plot predicted overlay
        axes[i, 0].imshow(pred_overlay)
        axes[i, 0].set_title(f"Predicted Overlay: Case {case_num}")
        axes[i, 0].axis('off')

        # Plot ground truth overlay
        axes[i, 1].imshow(gt_overlay)
        axes[i, 1].set_title(f"Ground Truth Overlay: Case {case_num}")
        axes[i, 1].axis('off')

    plt.tight_layout()
    plt.show()

def predict_and_visualize_mask(case_num, model, device, transform, image_dir, mask_dir):
    """
    Loads an image, predicts the segmentation mask using a trained U-Net model,
    resizes the mask to original image size, and visualizes it.

    Args:
        case_num (str/int): Case number of the image.
        model (torch.nn.Module): Trained segmentation model.
        device (torch.device): Device (CPU or GPU) to run inference.
        transform (torchvision.transforms): Preprocessing transform for the model.
        image_dir (str): Directory containing the images.
        mask_dir (str): Directory containing the masks.
    """
    # Load mask
    mask_path = os.path.join(mask_dir, f'{case_num}.png')
    mask =  Image.open(mask_path).convert('L')
    # Load image
    img_path = os.path.join(image_dir, f'{case_num}.png')
    img = Image.open(img_path).convert('RGB')
    img_np = np.array(img)
    if img_np.dtype != np.uint8:
        img_np = (img_np * 255).astype(np.uint8)

    pil_img = Image.fromarray(img_np)
    original_size = pil_img.size  # (width, height)

    # Prepare tensor
    input_tensor = transform(pil_img).unsqueeze(0).to(device)

    # Predict mask
    model.eval()
    with torch.no_grad():
        output = model(input_tensor)  # shape: (1, 2, H, W)

    output = output.squeeze(0)  # shape: (2, H, W)
    pred_mask = torch.argmax(output, dim=0).cpu().numpy()  # (H, W)

    # Resize to original size
    mask_pil = Image.fromarray(pred_mask.astype(np.uint8), mode='L')
    mask_resized = mask_pil.resize(original_size, resample=Image.NEAREST)
    mask_resized_np = np.array(mask_resized)

    # Display
    plt.figure(figsize=(6,6))
    plt.imshow(mask_resized_np, cmap='gray')
    plt.title(f"Predicted Mask for Case {case_num}")
    plt.axis('off')
    plt.show()

    return mask_resized_np, img, mask

test_case_list = [re.findall('\d{1,3}', x)[0] for x in os.listdir(img_dir)]
case_list_num = test_case_list[10:13]
masks_imgs_list = [predict_and_visualize_mask(c, CBAM_model, device, test_img_transform, img_dir, mask_dir) for c in case_list_num]

plot_pred_vs_gt_from_masks(case_list_num, masks_imgs_list)