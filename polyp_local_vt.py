# -*- coding: utf-8 -*-
"""polyp_Local_VT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RA8PBj_R0PPCYnZhsiMG3kSzK37yifjt
"""

!pip install einops

import os
from google.colab import drive
drive.mount('/content/drive')
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import cv2
from google.colab.patches import cv2_imshow as CV2_imshow
import torch
import torchvision
from torchvision import transforms
import torch.nn as nn
import torch.nn.functional as F
import re
from torch.utils.data import Dataset, DataLoader, random_split
from einops import rearrange
from torch.optim.lr_scheduler import ReduceLROnPlateau
from skimage.metrics import hausdorff_distance
from scipy.spatial.distance import cdist

img_dir = '/content/drive/MyDrive/Dissertation/polyp_datasets/CVC-ClinicDB/PNG/Original'
mask_dir = '/content/drive/MyDrive/Dissertation/polyp_datasets/CVC-ClinicDB/PNG/Ground Truth'
os.listdir(mask_dir)[1]

case_list = [re.findall('\d{1,3}', x)[0] for x in os.listdir(img_dir)]

def load_img_mask(case_num):

  img_path = os.path.join(img_dir, f'{case_num}.png')
  mask_path = os.path.join(mask_dir, f'{case_num}.png')

  img =  Image.open(img_path).convert('RGB')
  mask =  Image.open(mask_path).convert('L')

  return np.array(img), np.array(mask)

img, mask = load_img_mask(case_list[50])

def draw_mask(img, mask, scale):
  masked_image = img.copy()
  mask = np.expand_dims(mask,axis= 2)

  masked_image = np.where(mask.astype(int),
                          np.array([0,255,0], dtype = 'uint8'),
                          masked_image)

  masked_image = masked_image.astype(np.uint8)

  segmented_image = cv2.addWeighted(img, 0.8, masked_image, 0.2, 0)
  height, width = segmented_image.shape[:2]
  new_size = (int(scale * height), int(scale * width))

  resized = cv2.resize(segmented_image, new_size)
  CV2_imshow(resized)

draw_mask(img, mask, 0.5)

class SegmentationDataset(Dataset):
  def __init__(self, image_dir, mask_dir, img_transform = None, mask_transform = None):
    self.image_dir = image_dir
    self.mask_dir = mask_dir
    self.img_transform = img_transform
    self.mask_transform = mask_transform
    self.image_names = os.listdir(image_dir)

  def __len__(self):
    return len(self.image_names)

  def __getitem__(self, idx):
    match = re.findall('\d{1,3}', self.image_names[idx])
    img_path = os.path.join(self.image_dir, f'{match[0]}.png')
    mask_path = os.path.join(self.mask_dir, f'{match[0]}.png')

    img = Image.open(img_path).convert('RGB')
    if self.img_transform:
      img = self.img_transform(img)
    else:
      img = torch.from_numpy(np.array(img)).permute(2, 0, 1).float() / 255.0

    mask = Image.open(mask_path).convert('L')
    if self.mask_transform:
      mask = self.mask_transform(mask)
    else:
      mask = torch.from_numpy(np.array(mask)).long()

    return img, mask

test_set = SegmentationDataset(img_dir,mask_dir, img_transform= None, mask_transform= None)
len(test_set)

image_paths = [os.path.join(img_dir, f'{x}.png') for x in case_list]
image_paths[0]

to_tensor = transforms.ToTensor()

channel_sum = torch.zeros(3)
channel_squared_sum = torch.zeros(3)
num_pixels = 0

for img_path in image_paths:
  img = Image.open(img_path).convert('RGB')
  img_tensor = to_tensor(img)

  c,h,w = img_tensor.shape
  num_pixels += h * w

  channel_sum += img_tensor.sum(dim=[1,2])
  channel_squared_sum += (img_tensor ** 2).sum(dim=[1,2])


# Compute mean and std
mean = channel_sum / num_pixels
std = (channel_squared_sum / num_pixels - mean ** 2).sqrt()

print("Mean per channel:", mean)
print("Std per channel:", std)

img_transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.RandomRotation(degrees=15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.RandomAffine(degrees=0, translate=(0.1,0.1), scale=(0.9,1.1)),
    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.4008, 0.2694, 0.1841],
                         std=[0.2984, 0.2050, 0.1397])
])

mask_transform = transforms.Compose([
    transforms.Resize((256, 256), interpolation=Image.NEAREST),
    transforms.ToTensor(),
])

test_img_transform = transforms.Compose([
    transforms.Resize((256,256)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.4008, 0.2694, 0.1841],
                         std=[0.2984, 0.2050, 0.1397])
])


full_dataset = SegmentationDataset(img_dir, mask_dir, img_transform, mask_transform)

train_len = int(len(full_dataset) * 0.7)
val_len = int((len(full_dataset) - train_len)/2)
test_len = val_len

print(train_len, val_len)

generator = torch.Generator().manual_seed(100)
train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_len, val_len, test_len], generator)


train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=True)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, drop_last=True)
test_loader = DataLoader(test_dataset, batch_size = 8, shuffle= False)

class PatchEmbeddings(nn.Module):
    def __init__(self, in_channels, emb_size, patch_size):
        super().__init__()
        self.patch_size = patch_size
        self.proj = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        B, C, H, W = x.shape
        x = self.proj(x)  # (B, emb_size, H_patch, W_patch)
        H_patch, W_patch = x.shape[2], x.shape[3]
        x = x.flatten(2).transpose(1, 2)  # (B, N_patches, emb_size)
        return x, H_patch, W_patch


class PositionalEmbeddings(nn.Module):
    def __init__(self, seq_len, emb_size, base=0.02):
        super().__init__()
        self.pos_embed = nn.Parameter(torch.randn(1, seq_len, emb_size) * base)

    def forward(self, x):
        return x + self.pos_embed


class TransformerEncoderLayer(nn.Module):
    def __init__(self, emb_size, num_heads, mlp_ratio=4.0):
        super().__init__()
        self.norm1 = nn.LayerNorm(emb_size)
        self.attn = nn.MultiheadAttention(emb_size, num_heads, batch_first=True)
        self.norm2 = nn.LayerNorm(emb_size)
        self.mlp = nn.Sequential(
            nn.Linear(emb_size, int(emb_size*mlp_ratio)),
            nn.GELU(),
            nn.Linear(int(emb_size*mlp_ratio), emb_size)
        )

    def forward(self, x):
        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]
        x = x + self.mlp(self.norm2(x))
        return x


class VisionTransformerLocalAttention(nn.Module):
    def __init__(self, channels_in=3, emb_size=128, patch_size=(8,8),
                 img_size=(256,256), num_heads=8, n_layers=4, num_classes=3, window_size=3):
        super().__init__()
        self.patch_size = patch_size
        self.window_size = window_size
        seq_len = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])

        self.patch_embeddings = PatchEmbeddings(channels_in, emb_size, patch_size)
        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))
        self.positional_embeddings = PositionalEmbeddings(seq_len + 1, emb_size)
        self.transformer_encoders = nn.ModuleList([
            TransformerEncoderLayer(emb_size, num_heads) for _ in range(n_layers)
        ])
        self.patch_classifier = nn.Linear(emb_size, num_classes)

    def local_attention(self, x, H, W):
        B, N, C = x.shape
        x_2d = x.view(B, H, W, C)
        pad = self.window_size // 2
        x_padded = F.pad(x_2d, (0, 0, pad, pad, pad, pad))
        patches = []
        for i in range(H):
            for j in range(W):
                window = x_padded[:, i:i+self.window_size, j:j+self.window_size, :]
                window = window.reshape(B, -1, C)
                q = window
                k = window
                v = window
                scale = C ** 0.5
                attn = torch.softmax(torch.matmul(q, k.transpose(-2,-1)) / scale, dim=-1)
                out = torch.matmul(attn, v)
                center_idx = (self.window_size**2)//2
                patches.append(out[:, center_idx, :])
        x_local = torch.stack(patches, dim=1)
        return x_local

    def forward(self, x):
        B = x.size(0)
        x, H_patch, W_patch = self.patch_embeddings(x)
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat([x, cls_tokens], dim=1)
        x = self.positional_embeddings(x)

        for layer in self.transformer_encoders:
            x = layer(x)

        patch_tokens = x[:, :-1]
        cls_token = x[:, -1:]

        patch_tokens = self.local_attention(patch_tokens, H_patch, W_patch)

        patch_logits = self.patch_classifier(patch_tokens)
        patch_logits = patch_logits.permute(0, 2, 1).view(B, -1, H_patch, W_patch)
        seg_logits = F.interpolate(patch_logits, scale_factor=self.patch_size[0], mode='bilinear', align_corners=False)

        return seg_logits

class DiceLoss(nn.Module):
    def __init__(self, smooth=1e-5):
        super().__init__()
        self.smooth = smooth

    def forward(self, logits, masks):
        # Squeeze channel dimension if present
        masks = masks.squeeze(1)  # [B, H, W]
        probabilities = F.softmax(logits, dim=1)  # [B, C, H, W]
        masks = F.one_hot(masks.long(), num_classes=logits.shape[1])  # [B, H, W, C]
        masks = masks.permute(0, 3, 1, 2).float()  # [B, C, H, W]

        intersection = (probabilities * masks).sum(dim=(2,3))
        union = probabilities.sum(dim=(2,3)) + masks.sum(dim=(2,3))
        dice = (2 * intersection + self.smooth) / (union + self.smooth)
        dice_loss = 1 - dice.mean()

        return dice_loss


class multi_fn_loss(nn.Module):
  def __init__(self, dice_we, ce_we):
    '''A loss function that combines DiceLoss and CrossEntropyLoss for accurate segmentation
       Args:
          Dice_we - weight for DiceLoss
          ce_we - Weight for CrossEntropyLoss
    '''
    super().__init__()
    self.dice_we = dice_we
    self.ce_we = ce_we
    self.DiceLoss = DiceLoss()
    self.ce_loss = nn.CrossEntropyLoss()

  def forward(self, logits, masks):
    '''Args:
          Logits - Raw outputs from the model
          masks - Ground truth masks
       returns:
          Dice coefficient
    '''
    return (self.dice_we * self.DiceLoss(logits, masks)) + (self.ce_we * self.ce_loss(logits, masks))

def calculate_accuracy(preds, masks):
  ''' A function for calulcating the accuracy of a a model predictions by comparing the models output to the masks voxel by voxel
      Args:
          preds - Model outputs (logits) that contain the masks predictions
          masks - Ground truth masks
      return:
          The proportion of correctly classifed voxels compared to the total number of voxels
  '''
  predicted_classes = torch.argmax(preds, dim = 1)
  correct = (predicted_classes == masks).float().sum()
  total = torch.numel(masks)
  return correct/ total


def Dice_coef(preds, masks, smooth = 1e-5):
    '''Function for calculating the Dice coefficient metric
       Args:
          preds - Model outputs (logits) that contain the masks predictions
          masks - Ground truth masks
       returns:
          Dice coefficient metric
    '''

    # Converting the logits into probabilities and one hot encoding the masks
    probablities = F.softmax(preds, dim = 1)
    masks = F.one_hot(masks, num_classes=2).permute(0,3,1,2).float()

    # Calculating the intersecton between predictions and Ground truth
    intersection = (probablities * masks).sum(dim=(2,3))

    # Calculating total number of voxels in prediction and ground truth
    union = probablities.sum(dim=(2,3)) + masks.sum(dim=(2,3))

    # Calculating Dice coefficient metric
    dice = (2 * intersection + smooth)/(union + smooth)

    return dice.mean()

def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=15):
    ''' A function that sets up training and validation loops for the model
        Args:
            Model - Untrained model that will output raw logits
            Train_loader - SubjectsLoader that will load the training data into the training loop
            Val_loader - SubjectsLoader that will load validation data into the validation loop
            Criterion - The loss function that will be used adjust weights
            optimizer - The optimizer that will be used to adjust weights
            Device (Torch.device) - if the model will run on the GPU or CPU
            Num_epochs = How much epochs the training and validation loops will run for
        return:
            The model with the updated parameters and a dictionaly with model evaluation metrics
    '''
    model.to(device)

    # Scheduler to reduce learning rate if validation loss plateaus
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)

    # List to store model evaluation metrics
    final_train_loss = []
    final_val_loss = []
    final_train_accuracy = []
    final_val_accuracy = []
    final_train_dice = []
    final_val_dice = []

    for epoch in range(num_epochs):

        # List to store epoch evaluation metrics
        model.train()
        epoch_train_loss = []
        epoch_train_accuracy = []
        epoch_dice_score = []

        # Training loop
        for img, mask in train_loader:
            # Getting images and masks from train loader
            img, masks = img.to(device), mask.to(device)
            masks = masks.squeeze(1).long()

            # Model outputs raw logits used to calulcate loss and evaluation metrics
            outputs = model(img)
            loss = criterion(outputs, masks)
            acc = calculate_accuracy(outputs, masks)
            dice = Dice_coef(outputs, masks)

            # Optimize the weights
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # Appending evaluation metrics to respective lists
            epoch_train_loss.append(loss.item())
            epoch_train_accuracy.append(acc.cpu().item())
            epoch_dice_score.append(dice.cpu().item())

        # Averaging the respective lists
        avg_train_loss = sum(epoch_train_loss) / len(epoch_train_loss)
        avg_train_accuracy = sum(epoch_train_accuracy) / len(epoch_train_accuracy)
        avg_train_dice = sum(epoch_dice_score)/len(epoch_dice_score)

        # Appending the averages to the final lists that hold the scores for each epoch
        final_train_loss.append(avg_train_loss)
        final_train_accuracy.append(avg_train_accuracy)
        final_train_dice.append(avg_train_dice)

        # Setting the model to evaluation mode and creating lists to store validation metrics
        model.eval()
        epoch_val_loss = []
        epoch_val_accuracy = []
        epoch_val_dice = []

        # Setting up validation loops
        with torch.no_grad():
            for img, masks in val_loader:
                # Loading images and masks
                img, masks = img.to(device), mask.to(device)
                masks = masks.squeeze(1).long()

                # Model outputs (logits) used to calulcate loss, accuracy and dice Coefficent
                outputs = model(img)
                loss = criterion(outputs, masks)
                acc = calculate_accuracy(outputs, masks)
                dice = Dice_coef(outputs, masks)

                # Appending evaluation metrics to respective lists
                epoch_val_loss.append(loss.item())
                epoch_val_accuracy.append(acc.cpu().item())
                epoch_val_dice.append(dice.cpu().item())

        # Averaging the respective lists
        avg_val_loss = sum(epoch_val_loss) / len(epoch_val_loss)
        avg_val_accuracy = sum(epoch_val_accuracy) / len(epoch_val_accuracy)
        avg_val_dice = sum(epoch_val_dice) / len(epoch_val_dice)

        # Appending the averages to the final lists that hold the scores for each epoch
        final_val_loss.append(avg_val_loss)
        final_val_accuracy.append(avg_val_accuracy)
        final_val_dice.append(avg_val_dice)

        # Step the learning rate scheduler based on validation loss
        scheduler.step(avg_val_loss)

        # Printing the training and validation loss, accuracy and Dice score for each epoch
        print(f"Epoch [{epoch+1}/{num_epochs}]")
        print(f"  Train     - Loss: {avg_train_loss:.2f}, Accuracy: {avg_train_accuracy:.2f}, Dice: {avg_train_dice:.2f}")
        print(f"  Validation- Loss: {avg_val_loss:.2f}, Accuracy: {avg_val_accuracy:.2f}, Dice: {avg_val_dice:.2f}")

    # Returning a dictionary conatining lists with the evaluation metrics for each epoch
    return {
        'train_loss': final_train_loss,
        'val_loss': final_val_loss,
        'train_accuracy': final_train_accuracy,
        'val_accuracy': final_val_accuracy,
        'Train_dice': final_train_dice,
        'val_dice': final_val_dice
    }

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
VT_model = VisionTransformerLocalAttention(channels_in=3,
    emb_size=128,
    patch_size=(8,8),
    img_size=(256,256),
    num_heads=8,
    n_layers=6,
    num_classes=2,
    window_size= 3)
criterion = multi_fn_loss(0.95, 0.05)
optimiser = torch.optim.Adam(VT_model.parameters(), lr=1e-4)

VT_metrics = train_model(VT_model, train_loader, val_loader, criterion, optimiser, device, 25)

def plot_accuracy(train_acc, val_acc, title='Training and validation loss over epochs: VT_model'):
    ''' A function that plots model accuracy across training and validation epochs
        Args:
          train_acc - List with the model accuracy during training
          val_acc - list with the model accuracy during validation
          title - Optional string for plot title
        returns:
          A line plot of training and validation accuracy across epochs
    '''
    epochs = list(range(1, len(train_acc) + 1))
    plt.plot(epochs, train_acc, label='Training accuracy')
    plt.plot(epochs, val_acc, label='Validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend(loc='center right', bbox_to_anchor=(1, 0.7))
    plt.title(title if title else 'Training and validation accuracy over epochs')
    plt.tight_layout()
    plt.show()

def plot_loss(train_loss, val_loss, title='Training and validation loss over epochs: VT_model'):
    ''' A function that plots model loss across training and validation epochs
        Args:
          train_loss - List with the model loss during training
          val_loss - list with the model loss during validation
          title - Optional string for plot title
        returns:
          A line plot of training and validation loss across epochs
    '''
    epochs = list(range(1, len(train_loss) + 1))
    plt.plot(epochs, train_loss, label='Training loss')
    plt.plot(epochs, val_loss, label='Validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend(loc='best')
    plt.title(title if title else 'Training and validation loss over epochs')
    plt.tight_layout()
    plt.show()

def plot_dice(train_dice, val_dice, title='Training and validation loss over epochs: VT_model'):
    ''' A function that plots the Dice coefficient scores across training and validation epochs
        Args:
          train_dice - List with the Dice coefficient scores during training
          val_dice - list with the Dice coefficient scores during validation
          title - Optional string for plot title
        returns:
          A line plot of training and validation Dice coefficient scores across epochs
    '''
    epochs = list(range(1, len(train_dice) + 1))
    plt.plot(epochs, train_dice, label='Training Dice score')
    plt.plot(epochs, val_dice, label='Validation Dice score')
    plt.xlabel('Epochs')
    plt.ylabel('Dice score')
    plt.legend(loc='best')
    plt.title(title if title else 'Dice score over epochs')
    plt.tight_layout()
    plt.show()

plot_accuracy(VT_metrics['train_accuracy'],VT_metrics['val_accuracy'])
plot_loss(VT_metrics['train_loss'], VT_metrics['val_loss'])
plot_dice(VT_metrics['Train_dice'], VT_metrics['val_dice'])

def save_model(model, save_dir, filename="VT_local_model.pth"):
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, filename)
    torch.save(model.state_dict(), save_path)
    print(f"Model saved to {save_path}")


save_model(VT_model, '/content/drive/MyDrive/Dissertation/polyp_datasets/models')

VT_model = VisionTransformerLocalAttention(channels_in=3,
    emb_size=128,
    patch_size=(8,8),
    img_size=(256,256),
    num_heads=8,
    n_layers=6,
    num_classes=2,
    window_size= 3)

VT_model.load_state_dict(torch.load('/content/drive/MyDrive/Dissertation/polyp_datasets/models/VT_local_model.pth', map_location= torch.device('cpu')))

class Dice_coef(nn.Module):
    def __init__(self, smooth=1e-6):
        super(Dice_coef, self).__init__()
        self.smooth = smooth

    def forward(self, logits, masks):
        # Apply sigmoid to logits
        probabilities = torch.sigmoid(logits)

        # If masks are [B, H, W], unsqueeze to [B, 1, H, W]
        if masks.dim() == 3:
            masks = masks.unsqueeze(1)

        masks = masks.float().to(logits.device)

        # Dice coefficient calculation
        intersection = (probabilities * masks).sum(dim=(2, 3))
        union = probabilities.sum(dim=(2, 3)) + masks.sum(dim=(2, 3))
        dice = (2 * intersection + self.smooth) / (union + self.smooth)

        return dice.mean()


def test_model(model, test_loader, criterion, device):
    model.to(device)
    model.eval()

    dice_fn = Dice_coef().to(device)
    accuracy = []
    loss_list = []
    dice_list = []

    with torch.no_grad():
        for img, masks in test_loader:
            img, masks = img.to(device), masks.to(device)
            # For BCEWithLogitsLoss:
            # masks = masks.float()
            # For CrossEntropyLoss:
            masks = masks.long()  # if you use CrossEntropyLoss
            masks = masks.squeeze(1) # Squeeze the channel dimension

            outputs = model(img)  # (B, num_classes, H, W)
            loss = criterion(outputs, masks)

            acc = calculate_accuracy(outputs, masks)
            dice = dice_fn(outputs, masks)

            loss_list.append(loss.item())
            accuracy.append(acc.cpu().item())
            dice_list.append(dice.cpu().item())

    avg_accuracy = sum(accuracy) / len(accuracy)
    avg_loss = sum(loss_list) / len(loss_list)
    avg_dice = sum(dice_list) / len(dice_list)

    print(f'Testing: {type(model).__name__}')
    print(f'Accuracy: {avg_accuracy:.2f}, Loss: {avg_loss:.2f}, Dice: {avg_dice:.2f}')

test_model(VT_model, test_loader, criterion, device)

def test_model_metrics_skimage(model, test_loader, device):
    """
    Test a segmentation model and compute Jaccard (IoU), Hausdorff distance, and Volume Similarity.
    Uses skimage.metrics.hausdorff_distance for Hausdorff.
    """
    model.to(device)
    model.eval()

    jaccard_list = []
    hausdorff_list = []
    volume_similarity_list = []

    with torch.no_grad():
        for img, masks in test_loader:
            img, masks = img.to(device), masks.to(device)
            # Ensure masks shape [B, 1, H, W]
            if masks.dim() == 3:
                masks = masks.unsqueeze(1)
            masks = masks.float()

            outputs = model(img)
            probs = torch.sigmoid(outputs)
            preds = (probs > 0.5).float()

            # Convert to numpy for skimage
            preds_np = preds.cpu().numpy()
            masks_np = masks.cpu().numpy()

            for b in range(preds_np.shape[0]):
                pred_mask = preds_np[b,0]
                true_mask = masks_np[b,0]

                # Jaccard (IoU)
                intersection = np.logical_and(pred_mask, true_mask).sum()
                union = np.logical_or(pred_mask, true_mask).sum()
                jaccard = (intersection + 1e-6) / (union + 1e-6)
                jaccard_list.append(jaccard)

                # Hausdorff distance
                try:
                    hd = hausdorff_distance(pred_mask.astype(bool), true_mask.astype(bool))
                except ValueError:
                    hd = np.nan  # if one mask is empty
                hausdorff_list.append(hd)

                # Volume similarity
                vol_pred = pred_mask.sum()
                vol_true = true_mask.sum()
                vol_sim = 1 - abs(vol_pred - vol_true) / (vol_pred + vol_true + 1e-6)
                volume_similarity_list.append(vol_sim)

    avg_jaccard = np.nanmean(jaccard_list)
    avg_hausdorff = np.nanmean(hausdorff_list)
    avg_volsim = np.nanmean(volume_similarity_list)

    print(f'Testing: {type(model).__name__}')
    print(f'Jaccard: {avg_jaccard:.4f}, Hausdorff: {avg_hausdorff:.4f}, Volume Similarity: {avg_volsim:.4f}')

test_model_metrics_skimage(VT_model, test_loader, device)

def predict_and_visualize_mask(case_num, model, device, transform, image_dir, mask_dir):
    """
    Loads an image, predicts the segmentation mask using a trained U-Net model,
    resizes the mask to original image size, and visualizes it.

    Args:
        case_num (str/int): Case number of the image.
        model (torch.nn.Module): Trained segmentation model.
        device (torch.device): Device (CPU or GPU) to run inference.
        transform (torchvision.transforms): Preprocessing transform for the model.
        image_dir (str): Directory containing the images.
        mask_dir (str): Directory containing the masks.
    """
    # Load mask
    mask_path = os.path.join(mask_dir, f'{case_num}.png')
    mask =  Image.open(mask_path).convert('L')
    # Load image
    img_path = os.path.join(image_dir, f'{case_num}.png')
    img = Image.open(img_path).convert('RGB')
    img_np = np.array(img)
    if img_np.dtype != np.uint8:
        img_np = (img_np * 255).astype(np.uint8)

    pil_img = Image.fromarray(img_np)
    original_size = pil_img.size  # (width, height)

    # Prepare tensor
    input_tensor = transform(pil_img).unsqueeze(0).to(device)

    # Predict mask
    model.eval()
    with torch.no_grad():
        output = model(input_tensor)  # shape: (1, 2, H, W)

    output = output.squeeze(0)  # shape: (2, H, W)
    pred_mask = torch.argmax(output, dim=0).cpu().numpy()  # (H, W)

    # Resize to original size
    mask_pil = Image.fromarray(pred_mask.astype(np.uint8), mode='L')
    mask_resized = mask_pil.resize(original_size, resample=Image.NEAREST)
    mask_resized_np = np.array(mask_resized)

    # Display
    plt.figure(figsize=(6,6))
    plt.imshow(mask_resized_np, cmap='gray')
    plt.title(f"Predicted Mask for Case {case_num}")
    plt.axis('off')
    plt.show()

    return mask_resized_np, img, mask

def plot_pred_vs_gt_from_masks(case_list, masks_imgs_list, max_cases=5):
    """
    Plots predicted vs ground-truth mask overlays in subplots for multiple cases.

    Args:
        case_list (list): List of case numbers.
        masks_imgs_list (list of tuples): Each element is (pred_mask, img, gt_mask)
        max_cases (int): Maximum number of cases to display.
    """
    def overlay_mask_on_image(img, mask, color=[255, 0, 0], alpha=0.4):
        """Overlay a binary mask on an image."""
        if isinstance(img, Image.Image):
            img = np.array(img)
        overlay = img.copy()
        mask_bin = (mask > 0).astype(np.uint8)
        color_mask = np.zeros_like(img)
        color_mask[mask_bin == 1] = color
        cv2.addWeighted(color_mask, alpha, overlay, 1.0, 0, overlay)
        return overlay

    num_cases = min(len(case_list), max_cases)
    fig, axes = plt.subplots(num_cases, 2, figsize=(10, 5*num_cases))

    if num_cases == 1:
        axes = np.expand_dims(axes, axis=0)  # Ensure 2D indexing

    for i, case_num in enumerate(case_list[:num_cases]):
        pred_mask, img, gt_mask = masks_imgs_list[i]

        # Overlay masks
        pred_overlay = overlay_mask_on_image(img, pred_mask, color=[255, 0, 0], alpha=0.4)
        gt_overlay = overlay_mask_on_image(img, np.array(gt_mask), color=[0, 255, 0], alpha=0.4)

        # Plot predicted overlay
        axes[i, 0].imshow(pred_overlay)
        axes[i, 0].set_title(f"Predicted Overlay: Case {case_num}")
        axes[i, 0].axis('off')

        # Plot ground truth overlay
        axes[i, 1].imshow(gt_overlay)
        axes[i, 1].set_title(f"Ground Truth Overlay: Case {case_num}")
        axes[i, 1].axis('off')

    plt.tight_layout()
    plt.show()


test_case_list = [re.findall('\d{1,3}', x)[0] for x in os.listdir(img_dir)]
case_list_num = test_case_list[50:55]
masks_imgs_list = [predict_and_visualize_mask(c, VT_model, device, test_img_transform, img_dir, mask_dir) for c in case_list_num]

plot_pred_vs_gt_from_masks(case_list_num, masks_imgs_list)