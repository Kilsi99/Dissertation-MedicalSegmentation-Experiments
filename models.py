# -*- coding: utf-8 -*-
"""Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W7cW5vj1sUnOFPamNGZAuWDx1ZzrTe8Y
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from einops import rearrange

# Encoder block
class Encoder_block(nn.Module):
    def __init__(self, channels_in, channels_out):
        '''
        Args:
            channels_in - Number of input channels (e.g., 1 for grayscale, 3 for RGB).
            channels_out - Number of feature maps outputted by this block.
        '''
        super(Encoder_block, self).__init__()

        # Each encoder block: 2x (Conv + ReLU) â†’ MaxPool (downsample spatial dims H/W by 2)
        self.conv = nn.Sequential(
            nn.Conv2d(channels_in, channels_out, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels_out, channels_out, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True)
        )
        self.pool = nn.MaxPool2d(2, 2)

    def forward(self, x):
        '''
        Args:
            x (torch.Tensor) - Input tensor shape [B, C, H, W]
        Returns:
            x: Feature maps for skip connection
            p: Downsampled feature maps for next encoder stage
        '''
        x = self.conv(x)
        p = self.pool(x)
        return x, p


# Decoder block
class Decoder_block(nn.Module):
    def __init__(self, channels_in, skip_channels, channels_out):
        '''
        Args:
            channels_in: Input feature maps from bottleneck or previous decoder block
            skip_channels: Feature maps from encoder skip connection
            channels_out: Output feature maps
        '''
        super(Decoder_block, self).__init__()

        self.upsample = nn.ConvTranspose2d(channels_in, channels_out, kernel_size=2, stride=2)
        self.conv = nn.Sequential(
            nn.Conv2d(channels_out + skip_channels, channels_out, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels_out, channels_out, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True)
        )

    def forward(self, x, skip):
        '''
        Args:
            x - Features from bottleneck or previous decoder block
            skip - Encoder skip features
        Returns:
            Tensor [B, channels_out, H, W]
        '''
        x = self.upsample(x)
        x = torch.cat([x, skip], dim=1)
        x = self.conv(x)
        return x


# U-Net Architecture
class Unet_2D(nn.Module):
    def __init__(self, channels_in, channels_out):
        '''
        Args:
            channels_in - Input image channels (1 for grayscale, 3 for RGB)
            channels_out - Number of classes for segmentation
        '''
        super(Unet_2D, self).__init__()

        # Encoder
        self.Encoder_block1 = Encoder_block(channels_in, 64)
        self.Encoder_block2 = Encoder_block(64, 128)
        self.Encoder_block3 = Encoder_block(128, 256)
        self.Encoder_block4 = Encoder_block(256, 512)

        # Bottleneck
        self.bottle_neck = nn.Sequential(
            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True)
        )

        # Decoder
        self.decoder_block4 = Decoder_block(1024, 512, 512)
        self.decoder_block3 = Decoder_block(512, 256, 256)
        self.decoder_block2 = Decoder_block(256, 128, 128)
        self.decoder_block1 = Decoder_block(128, 64, 64)

        # Output layer
        self.output = nn.Conv2d(64, channels_out, kernel_size=1)

    def forward(self, x):
        '''
        Args:
            x (torch.Tensor): [B, C, H, W]
        Returns:
            Segmentation logits [B, channels_out, H, W]
        '''
        x1, p1 = self.Encoder_block1(x)
        x2, p2 = self.Encoder_block2(p1)
        x3, p3 = self.Encoder_block3(p2)
        x4, p4 = self.Encoder_block4(p3)

        x5 = self.bottle_neck(p4)

        d4 = self.decoder_block4(x5, x4)
        d3 = self.decoder_block3(d4, x3)
        d2 = self.decoder_block2(d3, x2)
        d1 = self.decoder_block1(d2, x1)

        output = self.output(d1)
        return output

x = torch.randn(8,3, 256,256)
unet_test = Unet_2D(3,32)
unet_test(x)

class channel_attention(nn.Module):
  def __init__(self, channels_in, reduction_ratio = 16):
    super(channel_attention, self).__init__()

    self.mlp = nn.Sequential(
        nn.Linear(channels_in, channels_in//reduction_ratio, bias = False),
        nn.ReLU(inplace = True),
        nn.Linear(channels_in// reduction_ratio, channels_in, bias = False)
    )
    self.avg_pool = nn.AdaptiveAvgPool2d(1)
    self.max_pool = nn.AdaptiveMaxPool2d(1)
    self.sigmoid = nn.Sigmoid()

  def forward(self, x):
    b,c,_,_= x.size()
    avg_out = self.mlp(self.avg_pool(x).view(b,c))
    max_out = self.mlp(self.max_pool(x).view(b,c))
    out = avg_out + max_out
    scale = self.sigmoid(out).view(b,c,1,1)
    return x * scale

class spatial_attention(nn.Module):
  def __init__(self, kernel_size = 7):
    super(spatial_attention, self).__init__()
    padding = (kernel_size -1) // 2

    self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)
    self.sigmoid = nn.Sigmoid()

  def forward(self, x):

    avg_out = torch.mean(x, dim=1, keepdim= True)
    max_out, _ = torch.max(x, dim = 1, keepdim= True)

    x_cat = torch.cat([avg_out, max_out], dim=1)

    attention = self.sigmoid(self.conv(x_cat))

    return x * attention

class CBAM(nn.Module):
  def __init__(self, channels_in, reduction_ratio = 16, kernel_size =7):
    super(CBAM, self).__init__()

    self.channel_attention = channel_attention(channels_in, reduction_ratio)
    self.spatial_attention = spatial_attention(kernel_size)

  def forward(self, x):
    x = self.channel_attention(x)
    x = self.spatial_attention(x)
    return x

# Creating a 2D unet Architecture with attention gates
class Encoder_block(nn.Module):
  def __init__(self, channels_in, channels_out):
    '''Args:
          channels_in - Number of channels inputed into the model or pevious encoder block.
          1 for greyscale CT scans (model input)
          Channels_out - Base numbers of feature maps that will be upscaled throughut the encder or
          passed to the decoder if its the final encoder block

    '''
    super(Encoder_block, self).__init__()

    # the encoder compresses spatial dimentions while increasing feature depth
    # Each encoder block consist of 2x (2D convolution and Relu activation) followed by Max pooling
    # Max pooling half spacial demintions - D/H/W
    self.conv = nn.Sequential(
        nn.Conv2d(channels_in, channels_out, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU(inplace = True),
        nn.Conv2d(channels_out, channels_out, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU(inplace = True)
        )
    self.pool = nn.MaxPool2d(2,2)
    self.CBAM = CBAM(channels_out)

  # Defining how data will move through the encoder
  def forward(self, x):
    '''Args:
          x (torch.Tensor) - Input Tensor shape [B, Channels_in, D, H, W]
          Returns - x: Channels_out number of feature maps that will go to decoder
                       via the skip connection
                    p: Downsampled Feature maps after pooling. These will travel down the encoder
    '''
    x = self.conv(x)
    x = self.CBAM(x)
    p = self.pool(x)
    return x , p

class attention_gate(nn.Module):
  def __init__(self, channels_in, skip_channels, channels_out):
    super(attention_gate, self).__init__()

    self.Wg = nn.Sequential(
        nn.Conv2d(channels_in, channels_out, kernel_size = 3, stride = 1, padding = 1),
        nn.BatchNorm2d(channels_out)
    )

    self.Ws = nn.Sequential(
        nn.Conv2d(skip_channels, channels_out, kernel_size = 3, stride = 1, padding = 1),
        nn.BatchNorm2d(channels_out)
    )

    self.ReLU = nn.ReLU(inplace = True)
    self.output = nn.Sequential(
        nn.Conv2d(channels_out, channels_out, kernel_size = 3, stride = 1, padding = 1),
        nn.Sigmoid()
    )

  def forward(self, x, skip):
    wg = self.Wg(x)
    ws = self.Ws(skip)
    out = self.ReLU(wg + ws)
    out = self.output(out)
    return out * skip

# Defining the decoder block - Upsamples back to the orginal size using ConvTranspose2D
# Each stage halves feature depth ad doubles spacial information
class Decoder_block(nn.Module):
  def __init__(self, channels_in, skip_channels, channels_out):
    '''Args:
          channels_in: Input feature maps from the bottleneck or previous decoder block
          Skip_channels: feature maps from the encoder coming through the skip connection
          channels_out: feature maps outputted to the next decder block or final output layer

    '''
    super(Decoder_block, self).__init__()

    self.upsample = nn.ConvTranspose2d(channels_in, channels_out, kernel_size=2, stride = 2)
    self.attention_gate = attention_gate(channels_out, skip_channels, channels_out)
    self.conv = nn.Sequential(
         nn.Conv2d(channels_out + skip_channels, channels_out, kernel_size=3, stride = 1, padding = 1),
         nn.ReLU(inplace = True),
         nn.Conv2d(channels_out, channels_out, kernel_size=3, stride = 1, padding = 1),
         nn.ReLU(inplace = True)
    )
    self.CBAM = CBAM(channels_out)

  def forward(self, x, skip):
    '''Args:
          x - Features maps from bottleneck or previous decoder block
          Skip - Features maps coming from the encoder via the skip connection
       returns:
          Upsampled feature maps the will be used in the next decoder block or ouput layer
    '''
    x = self.upsample(x)
    gated_skip = self.attention_gate(x, skip)
    x = torch.cat([x, gated_skip], dim=1)
    x = self.conv(x)
    x = self.CBAM(x)
    return x


# Defining 2D Unet architecture - Data flows through 4 encoder blocoks reducing spacial size (D/H/W)
# Skip connections send feature maps to decoder by max pooling in each block
# Decoder upsamples back to orginal image size, feature maps are downscaled
class full_Attention_unet(nn.Module):
  def __init__(self, channels_in, channels_out):
    '''Args:
          channels_in - num channels inputted into first encoder block, 1 for greyscale CT scan
          Channels_out - Inital number of feature maps that will be outputted by first encoder block.
          This will be upscaled during the encoder and and downscaled during the decoder.
    '''
    super(full_Attention_unet, self).__init__()

    # 4 encoder blocks - upscaled feature maps and reduces spacial deimentions of image
    # First 3 send features maps to corresponding decoder block, last sends feature maps to the bottlebeck
    self.Encoder_block1 = Encoder_block(channels_in, channels_out)
    self.Encoder_block2 = Encoder_block(channels_out, channels_out * 2)
    self.Encoder_block3 = Encoder_block(channels_out * 2, channels_out * 4)
    self.Encoder_block4 = Encoder_block(channels_out * 4, channels_out * 8)

    # Bottleneck learns compressed high-level features at the lowest resolution between encoder and decoder.
    self.bottle_neck = nn.Sequential(
        nn.Conv2d(channels_out * 8, channels_out * 16, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU(inplace = True),
        nn.Conv2d(channels_out * 16, channels_out * 16, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU()
    )

    # Decoder architecure unsamples image spacial dimentions and reduces feature maps number
    # 1st decoder block inputs feature maps from bottleneck, next 3 lays recieves feature maps
    # from previous decoder block. Feature then fed to output layer
    self.decoder_block4 = Decoder_block(channels_out * 16, channels_out * 8, channels_out * 8)
    self.decoder_block3 = Decoder_block(channels_out * 8, channels_out * 4, channels_out * 4)
    self.decoder_block2 = Decoder_block(channels_out * 4, channels_out * 2, channels_out * 2)
    self.decoder_block1 = Decoder_block(channels_out * 2, channels_out , channels_out)

    # Map decoder output to 3 output chanbels, blackground left hypothalamus and right hypothalamus
    self.output = nn.Conv2d(channels_out, 2, kernel_size=3, stride = 1, padding = 1)


  def forward(self, x):
    '''Args:
          x (Torch.Tensor) - input tensor of shape [B, channels_in, D,H,W]
       returns:
          Torch.Tensor: Segmentation logits of shape [Batch, 3, Depth, Height, widith]
                         3-class output - Background, left hypothalamus, right hypothalamus
    '''
    x1, p1 = self.Encoder_block1(x)
    x2, p2 = self.Encoder_block2(p1)
    x3, p3 = self.Encoder_block3(p2)
    x4, p4 = self.Encoder_block4(p3)

    x5 = self.bottle_neck(p4)

    d4 = self.decoder_block4(x5, x4)
    d3 = self.decoder_block3(d4, x3)
    d2 = self.decoder_block2(d3, x2)
    d1 = self.decoder_block1(d2, x1)

    output = self.output(d1)

    return output

CBAM_test = full_Attention_unet(3,32)
CBAM_test(x)

class PatchEmbeddings(nn.Module):
    def __init__(self, channels_in, emb_size, patch_size):
        super().__init__()
        self.partition = nn.Conv2d(channels_in, emb_size, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        x = self.partition(x)  # (B, emb_size, H_patch, W_patch)
        B, C, H, W = x.shape
        x = rearrange(x, 'b c h w -> b (h w) c')  # (B, N_patches, emb_size)
        return x, H, W  # Return height and width of patches for reshaping later


class PositionalEmbeddings(nn.Module):
    def __init__(self, seq_len, emb_size, base=10000):
        super().__init__()
        pos = torch.arange(0, seq_len, dtype=torch.float32).unsqueeze(1)  # (seq_len, 1)
        div_term = torch.exp(torch.arange(0, emb_size, 2).float() * -(np.log(base) / emb_size))

        pe = torch.zeros(seq_len, emb_size)
        pe[:, 0::2] = torch.sin(pos * div_term)
        pe[:, 1::2] = torch.cos(pos * div_term)
        pe = pe.unsqueeze(0)  # (1, seq_len, emb_size)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:, :x.size(1), :]


class Multihead(nn.Module):
    def __init__(self, num_heads, emb_size):
        super().__init__()
        assert emb_size % num_heads == 0
        self.num_heads = num_heads
        self.head_dim = emb_size // num_heads

        self.key = nn.Linear(emb_size, emb_size)
        self.query = nn.Linear(emb_size, emb_size)
        self.value = nn.Linear(emb_size, emb_size)
        self.out = nn.Linear(emb_size, emb_size)

    def forward(self, x):
        B, N, C = x.shape

        k = rearrange(self.key(x), 'b n (h d) -> b h n d', h=self.num_heads)
        q = rearrange(self.query(x), 'b n (h d) -> b h n d', h=self.num_heads)
        v = rearrange(self.value(x), 'b n (h d) -> b h n d', h=self.num_heads)

        scale = self.head_dim ** 0.5
        attn = torch.softmax(torch.matmul(q, k.transpose(-2, -1)) / scale, dim=-1)
        out = torch.matmul(attn, v)  # (B, h, N, d)
        out = rearrange(out, 'b h n d -> b n (h d)')
        out = self.out(out)
        return out


class TransformerEncoderLayer(nn.Module):
    def __init__(self, emb_size, num_heads, dropout=0.1):
        super().__init__()
        self.norm1 = nn.LayerNorm(emb_size)
        self.attn = Multihead(num_heads, emb_size)
        self.drop1 = nn.Dropout(dropout)

        self.norm2 = nn.LayerNorm(emb_size)
        self.mlp = nn.Sequential(
            nn.Linear(emb_size, emb_size * 4),
            nn.GELU(),
            nn.Linear(emb_size * 4, emb_size),
        )
        self.drop2 = nn.Dropout(dropout)

    def forward(self, x):
        x = x + self.drop1(self.attn(self.norm1(x)))
        x = x + self.drop2(self.mlp(self.norm2(x)))
        return x


class VisionTransformerSegmentation(nn.Module):
    def __init__(self, channels_in, emb_size, patch_size, img_size, base, num_heads, n_layers, num_classes):
        super().__init__()
        assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0
        assert emb_size % num_heads == 0

        self.patch_size = patch_size
        self.num_classes = num_classes

        seq_len = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])

        self.patch_embeddings = PatchEmbeddings(channels_in, emb_size, patch_size)
        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))
        self.positional_embeddings = PositionalEmbeddings(seq_len + 1, emb_size, base)
        self.transformer_encoders = nn.Sequential(*[
            TransformerEncoderLayer(emb_size, num_heads) for _ in range(n_layers)
        ])

        # Patch-wise classifier: output 2 logits per patch (background / lesion)
        self.patch_classifier = nn.Linear(emb_size, num_classes)

    def forward(self, x):
        B = x.size(0)
        x, H_patch, W_patch = self.patch_embeddings(x)  # (B, N_patches, emb_size)

        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, emb_size)
        x = torch.cat([x, cls_tokens], dim=1)          # (B, N_patches+1, emb_size)

        x = self.positional_embeddings(x)
        x = self.transformer_encoders(x)  # (B, N_patches+1, emb_size)

        patch_tokens = x[:, :-1]  # Remove cls token: (B, N_patches, emb_size)

        # Classify each patch
        patch_logits = self.patch_classifier(patch_tokens)  # (B, N_patches, num_classes)

        # Reshape to image grid: (B, num_classes, H_patch, W_patch)
        patch_logits = patch_logits.permute(0, 2, 1)  # (B, num_classes, N_patches)
        patch_logits = patch_logits.view(B, self.num_classes, H_patch, W_patch)

        # Upsample to original image size
        seg_logits = F.interpolate(patch_logits,
                                   scale_factor=self.patch_size[0],
                                   mode='bilinear',
                                   align_corners=False)
        return seg_logits


VT_test = VisionTransformerSegmentation(
    channels_in=3,
    emb_size=128,
    patch_size=(8,8),
    img_size=(256,256),
    base=10000,
    num_heads=8,
    n_layers=6,
    num_classes=2
)
VT_test(x)

class PatchEmbeddings(nn.Module):
    def __init__(self, in_channels, emb_size, patch_size):
        super().__init__()
        self.patch_size = patch_size
        self.proj = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        B, C, H, W = x.shape
        x = self.proj(x)  # (B, emb_size, H_patch, W_patch)
        H_patch, W_patch = x.shape[2], x.shape[3]
        x = x.flatten(2).transpose(1, 2)  # (B, N_patches, emb_size)
        return x, H_patch, W_patch


class PositionalEmbeddings(nn.Module):
    def __init__(self, seq_len, emb_size, base=0.02):
        super().__init__()
        self.pos_embed = nn.Parameter(torch.randn(1, seq_len, emb_size) * base)

    def forward(self, x):
        return x + self.pos_embed


class TransformerEncoderLayer(nn.Module):
    def __init__(self, emb_size, num_heads, mlp_ratio=4.0):
        super().__init__()
        self.norm1 = nn.LayerNorm(emb_size)
        self.attn = nn.MultiheadAttention(emb_size, num_heads, batch_first=True)
        self.norm2 = nn.LayerNorm(emb_size)
        self.mlp = nn.Sequential(
            nn.Linear(emb_size, int(emb_size*mlp_ratio)),
            nn.GELU(),
            nn.Linear(int(emb_size*mlp_ratio), emb_size)
        )

    def forward(self, x):
        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]
        x = x + self.mlp(self.norm2(x))
        return x


class VisionTransformerLocalAttention(nn.Module):
    def __init__(self, channels_in=3, emb_size=128, patch_size=(8,8),
                 img_size=(256,256), num_heads=8, n_layers=4, num_classes=3, window_size=3):
        super().__init__()
        self.patch_size = patch_size
        self.window_size = window_size
        seq_len = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])

        self.patch_embeddings = PatchEmbeddings(channels_in, emb_size, patch_size)
        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))
        self.positional_embeddings = PositionalEmbeddings(seq_len + 1, emb_size)
        self.transformer_encoders = nn.ModuleList([
            TransformerEncoderLayer(emb_size, num_heads) for _ in range(n_layers)
        ])
        self.patch_classifier = nn.Linear(emb_size, num_classes)

    def local_attention(self, x, H, W):
        B, N, C = x.shape
        x_2d = x.view(B, H, W, C)
        pad = self.window_size // 2
        x_padded = F.pad(x_2d, (0, 0, pad, pad, pad, pad))
        patches = []
        for i in range(H):
            for j in range(W):
                window = x_padded[:, i:i+self.window_size, j:j+self.window_size, :]
                window = window.reshape(B, -1, C)
                q = window
                k = window
                v = window
                scale = C ** 0.5
                attn = torch.softmax(torch.matmul(q, k.transpose(-2,-1)) / scale, dim=-1)
                out = torch.matmul(attn, v)
                center_idx = (self.window_size**2)//2
                patches.append(out[:, center_idx, :])
        x_local = torch.stack(patches, dim=1)
        return x_local

    def forward(self, x):
        B = x.size(0)
        x, H_patch, W_patch = self.patch_embeddings(x)
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat([x, cls_tokens], dim=1)
        x = self.positional_embeddings(x)

        for layer in self.transformer_encoders:
            x = layer(x)

        patch_tokens = x[:, :-1]
        cls_token = x[:, -1:]

        patch_tokens = self.local_attention(patch_tokens, H_patch, W_patch)

        patch_logits = self.patch_classifier(patch_tokens)
        patch_logits = patch_logits.permute(0, 2, 1).view(B, -1, H_patch, W_patch)
        seg_logits = F.interpolate(patch_logits, scale_factor=self.patch_size[0], mode='bilinear', align_corners=False)

        return seg_logits

VT_test = VisionTransformerLocalAttention(channels_in=3,
    emb_size=128,
    patch_size=(8,8),
    img_size=(256,256),
    num_heads=8,
    n_layers=6,
    num_classes=2,
    window_size= 3)

VT_test(x)

class Basic_Unet_3D(nn.Module):
  def __init__(self, channels_in, channels_out):
    '''Args:
          channels_in - Number of channels inputed into the model or pevious encoder block.
          1 for greyscale CT scans (model input)
          Channels_out - Base numbers of feature maps that will be upscaled throughut the encder or
          passed to the decoder if its the final encoder block

    '''
    super(Basic_Unet_3D, self).__init__()

    # the encoder compresses spatial dimentions while increasing feature depth
    # Each encoder block consist of 2x (3D convolution and Relu activation) followed by Max pooling
    # Max pooling half spacial demintions - D/H/W
    self.Encoder_block1 = Encoder_block(channels_in, channels_out)
    self.Encoder_block2 = Encoder_block(channels_out, channels_out * 2)
    self.Encoder_block3 = Encoder_block(channels_out * 2, channels_out * 4)
    self.Encoder_block4 = Encoder_block(channels_out * 4, channels_out * 8)

    # Bottleneck learns compressed high-level features at the lowest resolution between encoder and decoder.
    self.bottle_neck = nn.Sequential(
        nn.Conv3d(channels_out * 8, channels_out * 16, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU(inplace = True),
        nn.Conv3d(channels_out * 16, channels_out * 16, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU()
    )

    # Decoder architecure unsamples image spacial dimentions and reduces feature maps number
    # 1st decoder block inputs feature maps from bottleneck, next 3 lays recieves feature maps
    # from previous decoder block. Feature then fed to output layer
    self.decoder_block4 = Decoder_block(channels_out * 16, channels_out * 8, channels_out * 8)
    self.decoder_block3 = Decoder_block(channels_out * 8, channels_out * 4, channels_out * 4)
    self.decoder_block2 = Decoder_block(channels_out * 4, channels_out * 2, channels_out * 2)
    self.decoder_block1 = Decoder_block(channels_out * 2, channels_out , channels_out)

    # Map decoder output to 3 output chanbels, blackground left hypothalamus and right hypothalamus
    self.output = nn.Conv3d(channels_out, 3, kernel_size=3, stride = 1, padding = 1)


  def forward(self, x):
    '''Args:
          x (Torch.Tensor) - input tensor of shape [B, channels_in, D,H,W]
       returns:
          Torch.Tensor: Segmentation logits of shape [Batch, 3, Depth, Height, widith]
                         3-class output - Background, left hypothalamus and right hypothalamus
    '''
    x1, p1 = self.Encoder_block1(x)
    x2, p2 = self.Encoder_block2(p1)
    x3, p3 = self.Encoder_block3(p2)
    x4, p4 = self.Encoder_block4(p3)

    x5 = self.bottle_neck(p4)

    d4 = self.decoder_block4(x5, x4)
    d3 = self.decoder_block3(d4, x3)
    d2 = self.decoder_block2(d3, x2)
    d1 = self.decoder_block1(d2, x1)

    output = self.output(d1)

    return output

class channel_attention(nn.Module):
  def __init__(self, channels_in, reduction_ratio = 16):
    super(channel_attention, self).__init__()

    self.mlp = nn.Sequential(
        nn.Linear(channels_in, channels_in//reduction_ratio, bias = False),
        nn.ReLU(inplace = True),
        nn.Linear(channels_in// reduction_ratio, channels_in, bias = False)
    )
    self.avg_pool = nn.AdaptiveAvgPool3d(1)
    self.max_pool = nn.AdaptiveMaxPool3d(1)
    self.sigmoid = nn.Sigmoid()

  def forward(self, x):
    b,c,_,_,_= x.size()
    avg_out = self.mlp(self.avg_pool(x).view(b,c))
    max_out = self.mlp(self.max_pool(x).view(b,c))
    out = avg_out + max_out
    scale = self.sigmoid(out).view(b,c,1,1,1)
    return x * scale

class spatial_attention(nn.Module):
  def __init__(self, kernel_size = 7):
    super(spatial_attention, self).__init__()
    padding = (kernel_size -1) // 2

    self.conv = nn.Conv3d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)
    self.sigmoid = nn.Sigmoid()

  def forward(self, x):

    avg_out = torch.mean(x, dim=1, keepdim= True)
    max_out, _ = torch.max(x, dim = 1, keepdim= True)

    x_cat = torch.cat([avg_out, max_out], dim=1)

    attention = self.sigmoid(self.conv(x_cat))

    return x * attention

class CBAM(nn.Module):
  def __init__(self, channels_in, reduction_ratio = 16, kernel_size =7):
    super(CBAM, self).__init__()

    self.channel_attention = channel_attention(channels_in, reduction_ratio)
    self.spatial_attention = spatial_attention(kernel_size)

  def forward(self, x):
    x = self.channel_attention(x)
    x = self.spatial_attention(x)
    return x

# Creating a 3D unet Architecture with attention gates
class Encoder_block(nn.Module):
  def __init__(self, channels_in, channels_out):
    '''Args:
          channels_in - Number of channels inputed into the model or pevious encoder block.
          1 for greyscale CT scans (model input)
          Channels_out - Base numbers of feature maps that will be upscaled throughut the encder or
          passed to the decoder if its the final encoder block

    '''
    super(Encoder_block, self).__init__()

    # the encoder compresses spatial dimentions while increasing feature depth
    # Each encoder block consist of 2x (3D convolution and Relu activation) followed by Max pooling
    # Max pooling half spacial demintions - D/H/W
    self.conv = nn.Sequential(
        nn.Conv3d(channels_in, channels_out, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU(inplace = True),
        nn.Conv3d(channels_out, channels_out, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU(inplace = True)
        )
    self.pool = nn.MaxPool3d(2,2)
    self.CBAM = CBAM(channels_out)

  # Defining how data will move through the encoder
  def forward(self, x):
    '''Args:
          x (torch.Tensor) - Input Tensor shape [B, Channels_in, D, H, W]
          Returns - x: Channels_out number of feature maps that will go to decoder
                       via the skip connection
                    p: Downsampled Feature maps after pooling. These will travel down the encoder
    '''
    x = self.conv(x)
    x = self.CBAM(x)
    p = self.pool(x)
    return x , p

class attention_gate(nn.Module):
  def __init__(self, channels_in, skip_channels, channels_out):
    super(attention_gate, self).__init__()

    self.Wg = nn.Sequential(
        nn.Conv3d(channels_in, channels_out, kernel_size = 3, stride = 1, padding = 1),
        nn.BatchNorm3d(channels_out)
    )

    self.Ws = nn.Sequential(
        nn.Conv3d(skip_channels, channels_out, kernel_size = 3, stride = 1, padding = 1),
        nn.BatchNorm3d(channels_out)
    )

    self.ReLU = nn.ReLU(inplace = True)
    self.output = nn.Sequential(
        nn.Conv3d(channels_out, channels_out, kernel_size = 3, stride = 1, padding = 1),
        nn.Sigmoid()
    )

  def forward(self, x, skip):
    wg = self.Wg(x)
    ws = self.Ws(skip)
    out = self.ReLU(wg + ws)
    out = self.output(out)
    return out * skip

# Defining the decoder block - Upsamples back to the orginal size using ConvTranspose3D
# Each stage halves feature depth ad doubles spacial information
class Decoder_block(nn.Module):
  def __init__(self, channels_in, skip_channels, channels_out):
    '''Args:
          channels_in: Input feature maps from the bottleneck or previous decoder block
          Skip_channels: feature maps from the encoder coming through the skip connection
          channels_out: feature maps outputted to the next decder block or final output layer

    '''
    super(Decoder_block, self).__init__()

    self.upsample = nn.ConvTranspose3d(channels_in, channels_out, kernel_size=2, stride = 2)
    self.attention_gate = attention_gate(channels_out, skip_channels, channels_out)
    self.conv = nn.Sequential(
         nn.Conv3d(channels_out + skip_channels, channels_out, kernel_size=3, stride = 1, padding = 1),
         nn.ReLU(inplace = True),
         nn.Conv3d(channels_out, channels_out, kernel_size=3, stride = 1, padding = 1),
         nn.ReLU(inplace = True)
    )
    self.CBAM = CBAM(channels_out)

  def forward(self, x, skip):
    '''Args:
          x - Features maps from bottleneck or previous decoder block
          Skip - Features maps coming from the encoder via the skip connection
       returns:
          Upsampled feature maps the will be used in the next decoder block or ouput layer
    '''
    x = self.upsample(x)
    x = self.attention_gate(x , skip)
    x = torch.cat([x, skip], dim=1)
    x = self.conv(x)
    x = self.CBAM(x)
    return x


# Defining 3D Unet architecture - Data flows through 4 encoder blocoks reducing spacial size (D/H/W)
# Skip connections send feature maps to decoder by max pooling in each block
# Decoder upsamples back to orginal image size, feature maps are downscaled
class full_Attention_3D_unet(nn.Module):
  def __init__(self, channels_in, channels_out):
    '''Args:
          channels_in - num channels inputted into first encoder block, 1 for greyscale CT scan
          Channels_out - Inital number of feature maps that will be outputted by first encoder block.
          This will be upscaled during the encoder and and downscaled during the decoder.
    '''
    super(full_Attention_3D_unet, self).__init__()

    # 4 encoder blocks - upscaled feature maps and reduces spacial deimentions of image
    # First 3 send features maps to corresponding decoder block, last sends feature maps to the bottlebeck
    self.Encoder_block1 = Encoder_block(channels_in, channels_out)
    self.Encoder_block2 = Encoder_block(channels_out, channels_out * 2)
    self.Encoder_block3 = Encoder_block(channels_out * 2, channels_out * 4)
    self.Encoder_block4 = Encoder_block(channels_out * 4, channels_out * 8)

    # Bottleneck learns compressed high-level features at the lowest resolution between encoder and decoder.
    self.bottle_neck = nn.Sequential(
        nn.Conv3d(channels_out * 8, channels_out * 16, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU(inplace = True),
        nn.Conv3d(channels_out * 16, channels_out * 16, kernel_size= 3, stride = 1, padding = 1),
        nn.ReLU()
    )

    # Decoder architecure unsamples image spacial dimentions and reduces feature maps number
    # 1st decoder block inputs feature maps from bottleneck, next 3 lays recieves feature maps
    # from previous decoder block. Feature then fed to output layer
    self.decoder_block4 = Decoder_block(channels_out * 16, channels_out * 8, channels_out * 8)
    self.decoder_block3 = Decoder_block(channels_out * 8, channels_out * 4, channels_out * 4)
    self.decoder_block2 = Decoder_block(channels_out * 4, channels_out * 2, channels_out * 2)
    self.decoder_block1 = Decoder_block(channels_out * 2, channels_out , channels_out)

    # Map decoder output to 3 output chanbels, blackground left hypothalamus and right hypothalamus
    self.output = nn.Conv3d(channels_out, 3, kernel_size=3, stride = 1, padding = 1)


  def forward(self, x):
    '''Args:
          x (Torch.Tensor) - input tensor of shape [B, channels_in, D,H,W]
       returns:
          Torch.Tensor: Segmentation logits of shape [Batch, 3, Depth, Height, widith]
                         3-class output - Background, left hypothalamus, right hypothalamus
    '''
    x1, p1 = self.Encoder_block1(x)
    x2, p2 = self.Encoder_block2(p1)
    x3, p3 = self.Encoder_block3(p2)
    x4, p4 = self.Encoder_block4(p3)

    x5 = self.bottle_neck(p4)

    d4 = self.decoder_block4(x5, x4)
    d3 = self.decoder_block3(d4, x3)
    d2 = self.decoder_block2(d3, x2)
    d1 = self.decoder_block1(d2, x1)

    output = self.output(d1)

    return output