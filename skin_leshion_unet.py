# -*- coding: utf-8 -*-
"""Skin_leshion_unet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KWs_3jZBrtmDverICI_G7Om3IzZvIX2X
"""

import torch
 import torchvision
 from torchvision import transforms
 import torch.nn as nn
 import torch.nn.functional as F
 import os
 import re
 from google.colab import drive
 drive.mount('/content/drive')
 import torch.optim as optim
 import matplotlib.pyplot as plt
 from torch.utils.data import Dataset, DataLoader, random_split
 import numpy as np
 from torch.optim.lr_scheduler import ReduceLROnPlateau
 import nibabel as nib
 from PIL import Image
 import cv2
 from google.colab.patches import cv2_imshow as CV2_imshow
 from einops import rearrange
 from skimage.metrics import hausdorff_distance

train_image_dir = '/content/drive/MyDrive/Dissertation/Skin_lesion_segmentation/ISBI2016_ISIC_Part1_Training_Data/ISBI2016_ISIC_Part1_Training_Data'
train_mask_dir = '/content/drive/MyDrive/Dissertation/Skin_lesion_segmentation/ISBI2016_ISIC_Part1_Training_GroundTruth/ISBI2016_ISIC_Part1_Training_GroundTruth'
test_image_dir = '/content/drive/MyDrive/Dissertation/Skin_lesion_segmentation/ISBI2016_ISIC_Part1_Test_Data/ISBI2016_ISIC_Part1_Test_Data'
test_mask_dir = '/content/drive/MyDrive/Dissertation/Skin_lesion_segmentation/ISBI2016_ISIC_Part1_Test_GroundTruth/ISBI2016_ISIC_Part1_Test_GroundTruth'

case_list = [re.findall('\d{7}', x)[0] for x in os.listdir(train_image_dir)]
def load_img_mask(case_num):

  img_path = os.path.join(train_image_dir, f'ISIC_{case_num}.jpg')
  mask_path = os.path.join(train_mask_dir, f'ISIC_{case_num}_Segmentation.png')

  img =  Image.open(img_path).convert('RGB')
  mask =  Image.open(mask_path).convert('L')

  return np.array(img), np.array(mask)

img, mask = load_img_mask(case_list[800])

def display_label(img, mask):

  fig, ax = plt.subplots(1,2, figsize = (7,7))
  ax = ax.flatten()

  for i, x in enumerate([img, mask]):
    ax[i].imshow(x)
    ax[i].axis('off')

  plt.tight_layout()
  plt.show()


def draw_mask(img, mask, scale):
  masked_image = img.copy()
  mask = np.expand_dims(mask,axis= 2)

  masked_image = np.where(mask.astype(int),
                          np.array([255,0,255], dtype = 'uint8'),
                          masked_image)

  masked_image = masked_image.astype(np.uint8)

  segmented_image = cv2.addWeighted(img, 0.8, masked_image, 0.2, 0)
  height, width = segmented_image.shape[:2]
  new_size = (int(scale * height), int(scale * width))

  resized = cv2.resize(segmented_image, new_size)
  CV2_imshow(resized)


for case_num in case_list[10:20]:
  img, mask = load_img_mask(case_num)
  draw_mask(img, mask, 0.5)

img.shape

class SegmentationDataset(Dataset):
  def __init__(self, image_dir, mask_dir, img_transform = None, mask_transform = None):
    self.image_dir = image_dir
    self.mask_dir = mask_dir
    self.img_transform = img_transform
    self.mask_transform = mask_transform
    self.image_names = os.listdir(image_dir)

  def __len__(self):
    return len(self.image_names)

  def __getitem__(self, idx):
    match = re.findall('\d{7}', self.image_names[idx])
    img_path = os.path.join(self.image_dir, f'ISIC_{match[0]}.jpg')
    mask_path = os.path.join(self.mask_dir, f'ISIC_{match[0]}_Segmentation.png')

    img = Image.open(img_path).convert('RGB')
    if self.img_transform:
      img = self.img_transform(img)
    else:
      img = torch.from_numpy(np.array(img)).permute(2, 0, 1).float() / 255.0

    mask = Image.open(mask_path).convert('L')
    if self.mask_transform:
      mask = self.mask_transform(mask)
    else:
      mask = torch.from_numpy(np.array(mask)).long()

    return img, mask

test_set = SegmentationDataset(train_image_dir, train_mask_dir, img_transform= None, mask_transform= None)
len(test_set)

class Test_Dataset(Dataset):
  def __init__(self, image_dir, mask_dir, img_transform = None, mask_transform = None):
    self.image_dir = image_dir
    self.mask_dir = mask_dir
    self.img_transform = img_transform
    self.mask_transform = mask_transform
    self.image_names = os.listdir(image_dir)

  def __len__(self):
    return len(self.image_names)

  def __getitem__(self, idx):
    match = re.findall('\d{7}', self.image_names[idx])
    img_path = os.path.join(self.image_dir, f'ISIC_{match[0]}.jpg')
    mask_path = os.path.join(self.mask_dir, f'ISIC_{match[0]}_Segmentation.png')

    img = Image.open(img_path).convert('RGB')
    if self.img_transform:
      img = self.img_transform(img)
    else:
      img = torch.from_numpy(np.array(img)).permute(2, 0, 1).float() / 255.0

    mask = Image.open(mask_path).convert('L')
    if self.mask_transform:
      mask = self.mask_transform(mask)
    else:
      mask = torch.from_numpy(np.array(mask)).long()

    return img, mask

image_paths = [os.path.join(train_image_dir, f'ISIC_{x}.jpg') for x in case_list]
image_paths[0]

to_tensor = transforms.ToTensor()

channel_sum = torch.zeros(3)
channel_squared_sum = torch.zeros(3)
num_pixels = 0

for img_path in image_paths:
  img = Image.open(img_path).convert('RGB')
  img_tensor = to_tensor(img)

  c,h,w = img_tensor.shape
  num_pixels += h * w

  channel_sum += img_tensor.sum(dim=[1,2])
  channel_squared_sum += (img_tensor ** 2).sum(dim=[1,2])


# Compute mean and std
mean = channel_sum / num_pixels
std = (channel_squared_sum / num_pixels - mean ** 2).sqrt()

print("Mean per channel:", mean)
print("Std per channel:", std)

img_transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.RandomRotation(degrees=15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.RandomAffine(degrees=0, translate=(0.1,0.1), scale=(0.9,1.1)),
    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.7086, 0.6054, 0.5476],
                         std=[0.1575, 0.1413, 0.1740])
])

mask_transform = transforms.Compose([
    transforms.Resize((256, 256), interpolation=Image.NEAREST),
    transforms.ToTensor(),
])

test_img_transform = transforms.Compose([
    transforms.Resize((256,256)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.7086, 0.6054, 0.5476],
                         std=[0.1575, 0.1413, 0.1740])
])


full_dataset = SegmentationDataset(train_image_dir, train_mask_dir, img_transform, mask_transform)
test_dataset = Test_Dataset(test_image_dir, test_mask_dir, test_img_transform, mask_transform)

train_len = int(len(full_dataset) * 0.8)
val_len = len(full_dataset) - train_len

generator = torch.Generator().manual_seed(100)
train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_len, val_len], generator)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=True)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, drop_last=True)
test_loader = DataLoader(test_dataset, batch_size = 8, shuffle= False)

# Encoder block
class Encoder_block(nn.Module):
    def __init__(self, channels_in, channels_out):
        '''
        Args:
            channels_in - Number of input channels (e.g., 1 for grayscale, 3 for RGB).
            channels_out - Number of feature maps outputted by this block.
        '''
        super(Encoder_block, self).__init__()

        # Each encoder block: 2x (Conv + ReLU) â†’ MaxPool (downsample spatial dims H/W by 2)
        self.conv = nn.Sequential(
            nn.Conv2d(channels_in, channels_out, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels_out, channels_out, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True)
        )
        self.pool = nn.MaxPool2d(2, 2)

    def forward(self, x):
        '''
        Args:
            x (torch.Tensor) - Input tensor shape [B, C, H, W]
        Returns:
            x: Feature maps for skip connection
            p: Downsampled feature maps for next encoder stage
        '''
        x = self.conv(x)
        p = self.pool(x)
        return x, p


# Decoder block
class Decoder_block(nn.Module):
    def __init__(self, channels_in, skip_channels, channels_out):
        '''
        Args:
            channels_in: Input feature maps from bottleneck or previous decoder block
            skip_channels: Feature maps from encoder skip connection
            channels_out: Output feature maps
        '''
        super(Decoder_block, self).__init__()

        self.upsample = nn.ConvTranspose2d(channels_in, channels_out, kernel_size=2, stride=2)
        self.conv = nn.Sequential(
            nn.Conv2d(channels_out + skip_channels, channels_out, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels_out, channels_out, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True)
        )

    def forward(self, x, skip):
        '''
        Args:
            x - Features from bottleneck or previous decoder block
            skip - Encoder skip features
        Returns:
            Tensor [B, channels_out, H, W]
        '''
        x = self.upsample(x)
        x = torch.cat([x, skip], dim=1)
        x = self.conv(x)
        return x


# U-Net Architecture
class Unet_2D(nn.Module):
    def __init__(self, channels_in, channels_out):
        '''
        Args:
            channels_in - Input image channels (1 for grayscale, 3 for RGB)
            channels_out - Number of classes for segmentation
        '''
        super(Unet_2D, self).__init__()

        # Encoder
        self.Encoder_block1 = Encoder_block(channels_in, 64)
        self.Encoder_block2 = Encoder_block(64, 128)
        self.Encoder_block3 = Encoder_block(128, 256)
        self.Encoder_block4 = Encoder_block(256, 512)

        # Bottleneck
        self.bottle_neck = nn.Sequential(
            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True)
        )

        # Decoder
        self.decoder_block4 = Decoder_block(1024, 512, 512)
        self.decoder_block3 = Decoder_block(512, 256, 256)
        self.decoder_block2 = Decoder_block(256, 128, 128)
        self.decoder_block1 = Decoder_block(128, 64, 64)

        # Output layer
        self.output = nn.Conv2d(64, channels_out, kernel_size=1)

    def forward(self, x):
        '''
        Args:
            x (torch.Tensor): [B, C, H, W]
        Returns:
            Segmentation logits [B, channels_out, H, W]
        '''
        x1, p1 = self.Encoder_block1(x)
        x2, p2 = self.Encoder_block2(p1)
        x3, p3 = self.Encoder_block3(p2)
        x4, p4 = self.Encoder_block4(p3)

        x5 = self.bottle_neck(p4)

        d4 = self.decoder_block4(x5, x4)
        d3 = self.decoder_block3(d4, x3)
        d2 = self.decoder_block2(d3, x2)
        d1 = self.decoder_block1(d2, x1)

        output = self.output(d1)
        return output

class DiceLoss(nn.Module):
    def __init__(self, smooth=1e-5):
        super().__init__()
        self.smooth = smooth

    def forward(self, logits, masks):
        # Squeeze channel dimension if present
        masks = masks.squeeze(1)  # [B, H, W]
        probabilities = F.softmax(logits, dim=1)  # [B, C, H, W]
        masks = F.one_hot(masks.long(), num_classes=logits.shape[1])  # [B, H, W, C]
        masks = masks.permute(0, 3, 1, 2).float()  # [B, C, H, W]

        intersection = (probabilities * masks).sum(dim=(2,3))
        union = probabilities.sum(dim=(2,3)) + masks.sum(dim=(2,3))
        dice = (2 * intersection + self.smooth) / (union + self.smooth)
        dice_loss = 1 - dice.mean()

        return dice_loss


class multi_fn_loss(nn.Module):
  def __init__(self, dice_we, ce_we):
    '''A loss function that combines DiceLoss and CrossEntropyLoss for accurate segmentation
       Args:
          Dice_we - weight for DiceLoss
          ce_we - Weight for CrossEntropyLoss
    '''
    super().__init__()
    self.dice_we = dice_we
    self.ce_we = ce_we
    self.DiceLoss = DiceLoss()
    self.ce_loss = nn.CrossEntropyLoss()

  def forward(self, logits, masks):
    '''Args:
          Logits - Raw outputs from the model
          masks - Ground truth masks
       returns:
          Dice coefficient
    '''
    return (self.dice_we * self.DiceLoss(logits, masks)) + (self.ce_we * self.ce_loss(logits, masks))

def calculate_accuracy(preds, masks):
  ''' A function for calulcating the accuracy of a a model predictions by comparing the models output to the masks voxel by voxel
      Args:
          preds - Model outputs (logits) that contain the masks predictions
          masks - Ground truth masks
      return:
          The proportion of correctly classifed voxels compared to the total number of voxels
  '''
  predicted_classes = torch.argmax(preds, dim = 1)
  correct = (predicted_classes == masks).float().sum()
  total = torch.numel(masks)
  return correct/ total


def Dice_coef(preds, masks, smooth = 1e-5):
    '''Function for calculating the Dice coefficient metric
       Args:
          preds - Model outputs (logits) that contain the masks predictions
          masks - Ground truth masks
       returns:
          Dice coefficient metric
    '''

    # Converting the logits into probabilities and one hot encoding the masks
    probablities = F.softmax(preds, dim = 1)
    masks = F.one_hot(masks, num_classes=2).permute(0,3,1,2).float()

    # Calculating the intersecton between predictions and Ground truth
    intersection = (probablities * masks).sum(dim=(2,3))

    # Calculating total number of voxels in prediction and ground truth
    union = probablities.sum(dim=(2,3)) + masks.sum(dim=(2,3))

    # Calculating Dice coefficient metric
    dice = (2 * intersection + smooth)/(union + smooth)

    return dice.mean()

def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=15):
    ''' A function that sets up training and validation loops for the model
        Args:
            Model - Untrained model that will output raw logits
            Train_loader - SubjectsLoader that will load the training data into the training loop
            Val_loader - SubjectsLoader that will load validation data into the validation loop
            Criterion - The loss function that will be used adjust weights
            optimizer - The optimizer that will be used to adjust weights
            Device (Torch.device) - if the model will run on the GPU or CPU
            Num_epochs = How much epochs the training and validation loops will run for
        return:
            The model with the updated parameters and a dictionaly with model evaluation metrics
    '''
    model.to(device)

    # Scheduler to reduce learning rate if validation loss plateaus
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)

    # List to store model evaluation metrics
    final_train_loss = []
    final_val_loss = []
    final_train_accuracy = []
    final_val_accuracy = []
    final_train_dice = []
    final_val_dice = []

    for epoch in range(num_epochs):

        # List to store epoch evaluation metrics
        model.train()
        epoch_train_loss = []
        epoch_train_accuracy = []
        epoch_dice_score = []

        # Training loop
        for img, mask in train_loader:
            # Getting images and masks from train loader
            img, masks = img.to(device), mask.to(device)
            masks = masks.squeeze(1).long()

            # Model outputs raw logits used to calulcate loss and evaluation metrics
            outputs = model(img)
            loss = criterion(outputs, masks)
            acc = calculate_accuracy(outputs, masks)
            dice = Dice_coef(outputs, masks)

            # Optimize the weights
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # Appending evaluation metrics to respective lists
            epoch_train_loss.append(loss.item())
            epoch_train_accuracy.append(acc.cpu().item())
            epoch_dice_score.append(dice.cpu().item())

        # Averaging the respective lists
        avg_train_loss = sum(epoch_train_loss) / len(epoch_train_loss)
        avg_train_accuracy = sum(epoch_train_accuracy) / len(epoch_train_accuracy)
        avg_train_dice = sum(epoch_dice_score)/len(epoch_dice_score)

        # Appending the averages to the final lists that hold the scores for each epoch
        final_train_loss.append(avg_train_loss)
        final_train_accuracy.append(avg_train_accuracy)
        final_train_dice.append(avg_train_dice)

        # Setting the model to evaluation mode and creating lists to store validation metrics
        model.eval()
        epoch_val_loss = []
        epoch_val_accuracy = []
        epoch_val_dice = []

        # Setting up validation loops
        with torch.no_grad():
            for img, masks in val_loader:
                # Loading images and masks
                img, masks = img.to(device), mask.to(device)
                masks = masks.squeeze(1).long()

                # Model outputs (logits) used to calulcate loss, accuracy and dice Coefficent
                outputs = model(img)
                loss = criterion(outputs, masks)
                acc = calculate_accuracy(outputs, masks)
                dice = Dice_coef(outputs, masks)

                # Appending evaluation metrics to respective lists
                epoch_val_loss.append(loss.item())
                epoch_val_accuracy.append(acc.cpu().item())
                epoch_val_dice.append(dice.cpu().item())

        # Averaging the respective lists
        avg_val_loss = sum(epoch_val_loss) / len(epoch_val_loss)
        avg_val_accuracy = sum(epoch_val_accuracy) / len(epoch_val_accuracy)
        avg_val_dice = sum(epoch_val_dice) / len(epoch_val_dice)

        # Appending the averages to the final lists that hold the scores for each epoch
        final_val_loss.append(avg_val_loss)
        final_val_accuracy.append(avg_val_accuracy)
        final_val_dice.append(avg_val_dice)

        # Step the learning rate scheduler based on validation loss
        scheduler.step(avg_val_loss)

        # Printing the training and validation loss, accuracy and Dice score for each epoch
        print(f"Epoch [{epoch+1}/{num_epochs}]")
        print(f"  Train     - Loss: {avg_train_loss:.2f}, Accuracy: {avg_train_accuracy:.2f}, Dice: {avg_train_dice:.2f}")
        print(f"  Validation- Loss: {avg_val_loss:.2f}, Accuracy: {avg_val_accuracy:.2f}, Dice: {avg_val_dice:.2f}")

    # Returning a dictionary conatining lists with the evaluation metrics for each epoch
    return {
        'train_loss': final_train_loss,
        'val_loss': final_val_loss,
        'train_accuracy': final_train_accuracy,
        'val_accuracy': final_val_accuracy,
        'Train_dice': final_train_dice,
        'val_dice': final_val_dice
    }

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
Unet_model =  Unet_2D(3,2)
criterion = multi_fn_loss(0.9, 0.05)
optimiser = torch.optim.Adam(Unet_model.parameters(), lr=1e-4)

UNET_metrics = train_model(Unet_model, train_loader, val_loader, criterion, optimiser, device, 25)

def plot_accuracy(train_acc, val_acc, title='Training and validation loss over epochs: unet'):
    ''' A function that plots model accuracy across training and validation epochs
        Args:
          train_acc - List with the model accuracy during training
          val_acc - list with the model accuracy during validation
          title - Optional string for plot title
        returns:
          A line plot of training and validation accuracy across epochs
    '''
    epochs = list(range(1, len(train_acc) + 1))
    plt.plot(epochs, train_acc, label='Training accuracy')
    plt.plot(epochs, val_acc, label='Validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend(loc='center right', bbox_to_anchor=(1, 0.7))
    plt.title(title if title else 'Training and validation accuracy over epochs')
    plt.tight_layout()
    plt.show()

def plot_loss(train_loss, val_loss, title='Training and validation loss over epochs: unet'):
    ''' A function that plots model loss across training and validation epochs
        Args:
          train_loss - List with the model loss during training
          val_loss - list with the model loss during validation
          title - Optional string for plot title
        returns:
          A line plot of training and validation loss across epochs
    '''
    epochs = list(range(1, len(train_loss) + 1))
    plt.plot(epochs, train_loss, label='Training loss')
    plt.plot(epochs, val_loss, label='Validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend(loc='best')
    plt.title(title if title else 'Training and validation loss over epochs')
    plt.tight_layout()
    plt.show()

def plot_dice(train_dice, val_dice, title='Training and validation loss over epochs: unet'):
    ''' A function that plots the Dice coefficient scores across training and validation epochs
        Args:
          train_dice - List with the Dice coefficient scores during training
          val_dice - list with the Dice coefficient scores during validation
          title - Optional string for plot title
        returns:
          A line plot of training and validation Dice coefficient scores across epochs
    '''
    epochs = list(range(1, len(train_dice) + 1))
    plt.plot(epochs, train_dice, label='Training Dice score')
    plt.plot(epochs, val_dice, label='Validation Dice score')
    plt.xlabel('Epochs')
    plt.ylabel('Dice score')
    plt.legend(loc='best')
    plt.title(title if title else 'Dice score over epochs')
    plt.tight_layout()
    plt.show()

plot_accuracy(UNET_metrics['train_accuracy'],UNET_metrics['val_accuracy'])
plot_loss(UNET_metrics['train_loss'], UNET_metrics['val_loss'])
plot_dice(UNET_metrics['Train_dice'], UNET_metrics['val_dice'])

def save_model(model, save_dir, filename="unet_model.pth"):
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, filename)
    torch.save(model.state_dict(), save_path)
    print(f"Model saved to {save_path}")


save_model(Unet_model, '/content/drive/MyDrive/Dissertation/Skin_lesion_segmentation/models')

Unet_model = Unet_2D(3,2)
Unet_model.load_state_dict(torch.load('/content/drive/MyDrive/Dissertation/Skin_lesion_segmentation/models/unet_model.pth', map_location= torch.device('cpu')))

class Dice_coef(nn.Module):
    def __init__(self, smooth=1e-6):
        super(Dice_coef, self).__init__()
        self.smooth = smooth

    def forward(self, logits, masks):
        # Apply sigmoid to logits
        probabilities = torch.sigmoid(logits)

        # If masks are [B, H, W], unsqueeze to [B, 1, H, W]
        if masks.dim() == 3:
            masks = masks.unsqueeze(1)

        masks = masks.float().to(logits.device)

        # Dice coefficient calculation
        intersection = (probabilities * masks).sum(dim=(2, 3))
        union = probabilities.sum(dim=(2, 3)) + masks.sum(dim=(2, 3))
        dice = (2 * intersection + self.smooth) / (union + self.smooth)

        return dice.mean()


def test_model(model, test_loader, criterion, device):
    model.to(device)
    model.eval()

    dice_fn = Dice_coef().to(device)
    accuracy = []
    loss_list = []
    dice_list = []

    with torch.no_grad():
        for img, masks in test_loader:
            img, masks = img.to(device), masks.to(device)
            masks = masks.squeeze(1).long()

            outputs = model(img)
            loss = criterion(outputs, masks)
            acc = calculate_accuracy(outputs, masks)
            dice = dice_fn(outputs, masks)

            loss_list.append(loss.item())
            accuracy.append(acc.cpu().item())
            dice_list.append(dice.cpu().item())

    avg_accuracy = sum(accuracy) / len(accuracy)
    avg_loss = sum(loss_list) / len(loss_list)
    avg_dice = sum(dice_list) / len(dice_list)

    print(f'Testing: {type(model).__name__}')
    print(f'Accuracy: {avg_accuracy:.2f}, Loss: {avg_loss:.2f}, Dice: {avg_dice:.2f}')

test_model(Unet_model, test_loader, criterion, device)

def test_model_metrics_skimage(model, test_loader, device):
    """
    Test a segmentation model and compute Jaccard (IoU), Hausdorff distance, and Volume Similarity.
    Uses skimage.metrics.hausdorff_distance for Hausdorff.
    """
    model.to(device)
    model.eval()

    jaccard_list = []
    hausdorff_list = []
    volume_similarity_list = []

    with torch.no_grad():
        for img, masks in test_loader:
            img, masks = img.to(device), masks.to(device)
            # Ensure masks shape [B, 1, H, W]
            if masks.dim() == 3:
                masks = masks.unsqueeze(1)
            masks = masks.float()

            outputs = model(img)
            probs = torch.sigmoid(outputs)
            preds = (probs > 0.5).float()

            # Convert to numpy for skimage
            preds_np = preds.cpu().numpy()
            masks_np = masks.cpu().numpy()

            for b in range(preds_np.shape[0]):
                pred_mask = preds_np[b,0]
                true_mask = masks_np[b,0]

                # Jaccard (IoU)
                intersection = np.logical_and(pred_mask, true_mask).sum()
                union = np.logical_or(pred_mask, true_mask).sum()
                jaccard = (intersection + 1e-6) / (union + 1e-6)
                jaccard_list.append(jaccard)

                # Hausdorff distance
                try:
                    hd = hausdorff_distance(pred_mask.astype(bool), true_mask.astype(bool))
                except ValueError:
                    hd = np.nan  # if one mask is empty
                hausdorff_list.append(hd)

                # Volume similarity
                vol_pred = pred_mask.sum()
                vol_true = true_mask.sum()
                vol_sim = 1 - abs(vol_pred - vol_true) / (vol_pred + vol_true + 1e-6)
                volume_similarity_list.append(vol_sim)

    avg_jaccard = np.nanmean(jaccard_list)
    avg_hausdorff = np.nanmean(hausdorff_list)
    avg_volsim = np.nanmean(volume_similarity_list)

    print(f'Testing: {type(model).__name__}')
    print(f'Jaccard: {avg_jaccard:.4f}, Hausdorff: {avg_hausdorff:.4f}, Volume Similarity: {avg_volsim:.4f}')

test_model_metrics_skimage(Unet_model, test_loader, device)

def predict_and_visualize_mask(case_num, model, device, transform, image_dir, mask_dir):
    """
    Loads an image, predicts the segmentation mask using a trained U-Net model,
    resizes the mask to original image size, and visualizes it.

    Args:
        case_num (str/int): Case number of the image.
        model (torch.nn.Module): Trained segmentation model.
        device (torch.device): Device (CPU or GPU) to run inference.
        transform (torchvision.transforms): Preprocessing transform for the model.
        image_dir (str): Directory containing the images.
        mask_dir (str): Directory containing the masks.
    """
    # Load mask
    mask_path = os.path.join(mask_dir, f'ISIC_{case_num}_Segmentation.png')
    mask =  Image.open(mask_path).convert('L')
    # Load image
    img_path = os.path.join(image_dir, f'ISIC_{case_num}.jpg')
    img = Image.open(img_path).convert('RGB')
    img_np = np.array(img)
    if img_np.dtype != np.uint8:
        img_np = (img_np * 255).astype(np.uint8)

    pil_img = Image.fromarray(img_np)
    original_size = pil_img.size  # (width, height)

    # Prepare tensor
    input_tensor = transform(pil_img).unsqueeze(0).to(device)

    # Predict mask
    model.eval()
    with torch.no_grad():
        output = model(input_tensor)  # shape: (1, 2, H, W)

    output = output.squeeze(0)  # shape: (2, H, W)
    pred_mask = torch.argmax(output, dim=0).cpu().numpy()  # (H, W)

    # Resize to original size
    mask_pil = Image.fromarray(pred_mask.astype(np.uint8), mode='L')
    mask_resized = mask_pil.resize(original_size, resample=Image.NEAREST)
    mask_resized_np = np.array(mask_resized)

    # Display
    plt.figure(figsize=(6,6))
    plt.imshow(mask_resized_np, cmap='gray')
    plt.title(f"Predicted Mask for Case {case_num}")
    plt.axis('off')
    plt.show()

    return mask_resized_np, img, mask

def plot_pred_vs_gt_from_masks(case_list, masks_imgs_list, max_cases=5):
    """
    Plots predicted vs ground-truth mask overlays in subplots for multiple cases.

    Args:
        case_list (list): List of case numbers.
        masks_imgs_list (list of tuples): Each element is (pred_mask, img, gt_mask)
        max_cases (int): Maximum number of cases to display.
    """
    def overlay_mask_on_image(img, mask, color=[255, 0, 0], alpha=0.4):
        """Overlay a binary mask on an image."""
        if isinstance(img, Image.Image):
            img = np.array(img)
        overlay = img.copy()
        mask_bin = (mask > 0).astype(np.uint8)
        color_mask = np.zeros_like(img)
        color_mask[mask_bin == 1] = color
        cv2.addWeighted(color_mask, alpha, overlay, 1.0, 0, overlay)
        return overlay

    num_cases = min(len(case_list), max_cases)
    fig, axes = plt.subplots(num_cases, 2, figsize=(10, 5*num_cases))

    if num_cases == 1:
        axes = np.expand_dims(axes, axis=0)  # Ensure 2D indexing

    for i, case_num in enumerate(case_list[:num_cases]):
        pred_mask, img, gt_mask = masks_imgs_list[i]

        # Overlay masks
        pred_overlay = overlay_mask_on_image(img, pred_mask, color=[255, 0, 0], alpha=0.4)
        gt_overlay = overlay_mask_on_image(img, np.array(gt_mask), color=[0, 255, 0], alpha=0.4)

        # Plot predicted overlay
        axes[i, 0].imshow(pred_overlay)
        axes[i, 0].set_title(f"Predicted Overlay: Case {case_num}")
        axes[i, 0].axis('off')

        # Plot ground truth overlay
        axes[i, 1].imshow(gt_overlay)
        axes[i, 1].set_title(f"Ground Truth Overlay: Case {case_num}")
        axes[i, 1].axis('off')

    plt.tight_layout()
    plt.show()

test_case_list = [re.findall('\d{7}', x)[0] for x in os.listdir(test_image_dir)]
case_list_num = test_case_list[0:4]
masks_imgs_list = [predict_and_visualize_mask(c, Unet_model, device, test_img_transform, test_image_dir, test_mask_dir) for c in case_list_num]

plot_pred_vs_gt_from_masks(case_list_num, masks_imgs_list)